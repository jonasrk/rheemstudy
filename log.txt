[INFO] Scanning for projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building rheemstudy 1.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- exec-maven-plugin:1.6.0:java (default-cli) @ rheemstudy ---
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.api.Configuration - Using blank configuration.
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.util.ReflectionUtils - Class class kmeansUnrolled$ is not loaded from a JAR file, but from /home/jonas.kemper/rheemstudy/target/classes/. Thus, cannot provide the JAR file.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.api.Job - Preparing plan...
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.plan.rheemplan.RheemPlan - Pruning unreachable Filter[Filter unstable centroids] from Rheem plan.
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.plan.rheemplan.RheemPlan - Pruning unreachable Map[1+1->1, id=20cabe35] from Rheem plan.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.api.Job - Estimating cardinalities and execution load...
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Using fallback selectivity for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@7053b561].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Using fallback selectivity for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@6ac4783b].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Using fallback selectivity for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@83a2a6b].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Using fallback selectivity for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@2373c55b].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Using fallback selectivity for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@749953ee].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Using fallback selectivity for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@58bf03c2].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Using fallback selectivity for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@1233474].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Using fallback selectivity for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@1e9f20de].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Using fallback selectivity for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@614947cb].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Using fallback selectivity for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@4ccdd2a3].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Using fallback selectivity for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@1453414a].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Using fallback selectivity for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@1db46d0].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Using fallback selectivity for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@4936ac65].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Using fallback selectivity for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@3f8a9d37].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Using fallback selectivity for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@b6c3e71].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Using fallback selectivity for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@618c8bf2].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.util.fs.HadoopFileSystem - Adding handler for HDFS URLs.
[kmeansUnrolled.main()] WARN org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[org.qcri.rheem.api.package$$anon$2@1429ff8d].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[kmeansUnrolled$SelectNearestCentroidForPoint$1@54cd6bfb].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for ReduceDescriptor[org.qcri.rheem.api.package$$anon$1@6c00ed42].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[org.qcri.rheem.api.package$$anon$2@64bd2cc9].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[kmeansUnrolled$TagStableCentroids$1@6b55ce7c].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@7053b561].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@6ac4783b].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[kmeansUnrolled$TagStablePoints$1@2a100388].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@618c8bf2].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[kmeansUnrolled$SelectNearestCentroidForPoint$1@58693475].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for ReduceDescriptor[org.qcri.rheem.api.package$$anon$1@6c2ea171].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[org.qcri.rheem.api.package$$anon$2@45a8c6b1].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[kmeansUnrolled$TagStableCentroids$1@6d5b0d7f].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@83a2a6b].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@2373c55b].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[kmeansUnrolled$TagStablePoints$1@79e6e9f1].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@b6c3e71].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[kmeansUnrolled$SelectNearestCentroidForPoint$1@2bf4e496].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for ReduceDescriptor[org.qcri.rheem.api.package$$anon$1@12b79c6d].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[org.qcri.rheem.api.package$$anon$2@d28968d].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[kmeansUnrolled$TagStableCentroids$1@77ecb452].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@749953ee].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@58bf03c2].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[kmeansUnrolled$TagStablePoints$1@82b35f0].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@3f8a9d37].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[kmeansUnrolled$SelectNearestCentroidForPoint$1@4c1dfc8c].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for ReduceDescriptor[org.qcri.rheem.api.package$$anon$1@e5f18f].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[org.qcri.rheem.api.package$$anon$2@5a9a77c2].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[kmeansUnrolled$TagStableCentroids$1@573d73b3].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@1233474].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@1e9f20de].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[kmeansUnrolled$TagStablePoints$1@3d7d6baa].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@4936ac65].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[kmeansUnrolled$SelectNearestCentroidForPoint$1@5a5a88d5].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for ReduceDescriptor[org.qcri.rheem.api.package$$anon$1@2aab3864].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[org.qcri.rheem.api.package$$anon$2@79d6bbb9].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[kmeansUnrolled$TagStableCentroids$1@62d85abd].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@614947cb].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@4ccdd2a3].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[kmeansUnrolled$TagStablePoints$1@1ddbe852].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@1db46d0].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[kmeansUnrolled$SelectNearestCentroidForPoint$1@12a9bcde].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for ReduceDescriptor[org.qcri.rheem.api.package$$anon$1@3cc85cad].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[org.qcri.rheem.api.package$$anon$2@229d091c].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for TransformationDescriptor[kmeansUnrolled$TagStableCentroids$1@2be0b2fa].
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.api.configuration.FunctionalKeyValueProvider - Creating fallback load estimator for PredicateDescriptor[org.qcri.rheem.api.package$$anon$4@1453414a].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.api.Job - Enumerating execution plans...
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.api.Job - Picked PlanImplementation[[Platform[Apache Spark]], (0:00:06.056 .. 0:00:06.106, p=3.34%), costs=(6,056.00..6,106.00 ~ 3.3%)] as best plan.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.api.Job - Compiling execution plan...
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.api.Job - Current execution plan:
>>> ExecutionStage[T[SparkCollectionSource[Load random centroids]]]:
>     SparkCollectionSource[Load random centroids] => RddChannel => SparkCollect[convert output@SparkCollectionSource[Load random centroids]]
> Out SparkCollect[convert output@SparkCollectionSource[Load random centroids]] => CollectionChannel

>>> ExecutionStage[T[SparkTextFileSource[Read file]]]:
>     SparkTextFileSource[Read file] => RddChannel => SparkMap[Create points]
> Out SparkMap[Create points] => RddChannel

>>> ExecutionStage[T[SparkBroadcast[convert output@SparkCollectionSource[Load random centroids]]]]:
> In  CollectionChannel => SparkBroadcast[convert output@SparkCollectionSource[Load random centroids]]
> Out SparkBroadcast[convert output@SparkCollectionSource[Load random centroids]] => BroadcastChannel
> Out SparkBroadcast[convert output@SparkCollectionSource[Load random centroids]] => BroadcastChannel

>>> ExecutionStage[T[SparkMap[Find nearest centroid - iteration zero]]]:
> In  RddChannel => SparkMap[Find nearest centroid - iteration zero]
> In  BroadcastChannel => SparkMap[Find nearest centroid - iteration zero]
>     SparkMap[Find nearest centroid - iteration zero] => RddChannel => SparkCache[convert out@SparkMap[Find nearest centroid - iteration zero]]
> Out SparkCache[convert out@SparkMap[Find nearest centroid - iteration zero]] => RddChannel
> Out SparkCache[convert out@SparkMap[Find nearest centroid - iteration zero]] => RddChannel

>>> ExecutionStage[T[SparkReduceBy[Add up points - iteration zero]]]:
> In  RddChannel => SparkReduceBy[Add up points - iteration zero]
>     SparkReduceBy[Add up points - iteration zero] => RddChannel => SparkMap[Average points - iteration zero]
>     SparkMap[Average points - iteration zero] => RddChannel => SparkMap[Tag stable centroids - iteration zero]
>     SparkMap[Tag stable centroids - iteration zero] => RddChannel => SparkCache[convert out@SparkMap[Tag stable centroids - iteration zero]]
> Out SparkCache[convert out@SparkMap[Tag stable centroids - iteration zero]] => RddChannel
> Out SparkCache[convert out@SparkMap[Tag stable centroids - iteration zero]] => RddChannel

>>> ExecutionStage[T[SparkMap[1+1->1, id=5e7550b5]]]:
> In  RddChannel => SparkMap[1+1->1, id=5e7550b5]
> In  BroadcastChannel => SparkMap[1+1->1, id=5e7550b5]
>     SparkMap[1+1->1, id=5e7550b5] => RddChannel => SparkFilter[1->1, id=5b7781d]
>     SparkFilter[1->1, id=5b7781d] => RddChannel => SparkMap[Find nearest centroid]
>     SparkMap[Find nearest centroid] => RddChannel => SparkCache[convert out@SparkMap[Find nearest centroid]]
> Out SparkCache[convert out@SparkMap[Find nearest centroid]] => RddChannel
> Out SparkCache[convert out@SparkMap[Find nearest centroid]] => RddChannel

>>> ExecutionStage[T[SparkFilter[Filter stable centroids - iteration zero]]]:
> In  RddChannel => SparkFilter[Filter stable centroids - iteration zero]
> Out SparkFilter[Filter stable centroids - iteration zero] => RddChannel

>>> ExecutionStage[T[SparkFilter[Filter unstable centroids - iteration zero]]]:
> In  RddChannel => SparkFilter[Filter unstable centroids - iteration zero]
>     SparkFilter[Filter unstable centroids - iteration zero] => RddChannel => SparkCollect[convert out@SparkFilter[Filter unstable centroids - iteration zero]]
> Out SparkCollect[convert out@SparkFilter[Filter unstable centroids - iteration zero]] => CollectionChannel

>>> ExecutionStage[T[SparkMap[1+1->1, id=757832fd]]]:
> In  RddChannel => SparkMap[1+1->1, id=757832fd]
> In  BroadcastChannel => SparkMap[1+1->1, id=757832fd]
>     SparkMap[1+1->1, id=757832fd] => RddChannel => SparkFilter[1->1, id=3db9b0a1]
>     SparkFilter[1->1, id=3db9b0a1] => RddChannel => SparkMap[Find nearest centroid]
>     SparkMap[Find nearest centroid] => RddChannel => SparkCache[convert out@SparkMap[Find nearest centroid]]
> Out SparkCache[convert out@SparkMap[Find nearest centroid]] => RddChannel
> Out SparkCache[convert out@SparkMap[Find nearest centroid]] => RddChannel

>>> ExecutionStage[T[SparkReduceBy[Add up points]]]:
> In  RddChannel => SparkReduceBy[Add up points]
>     SparkReduceBy[Add up points] => RddChannel => SparkMap[Average points]
>     SparkMap[Average points] => RddChannel => SparkMap[Tag stable centroids]
>     SparkMap[Tag stable centroids] => RddChannel => SparkCache[convert out@SparkMap[Tag stable centroids]]
> Out SparkCache[convert out@SparkMap[Tag stable centroids]] => RddChannel
> Out SparkCache[convert out@SparkMap[Tag stable centroids]] => RddChannel

>>> ExecutionStage[T[SparkUnionAll[2->1, id=2ef8007e]]]:
> In  RddChannel => SparkUnionAll[2->1, id=2ef8007e]
> In  RddChannel => SparkUnionAll[2->1, id=2ef8007e]
> Out SparkUnionAll[2->1, id=2ef8007e] => RddChannel

>>> ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids - iteration zero]]]]:
> In  CollectionChannel => SparkBroadcast[convert out@SparkFilter[Filter unstable centroids - iteration zero]]
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids - iteration zero]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids - iteration zero]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids - iteration zero]] => BroadcastChannel

>>> ExecutionStage[T[SparkReduceBy[Add up points]]]:
> In  RddChannel => SparkReduceBy[Add up points]
>     SparkReduceBy[Add up points] => RddChannel => SparkMap[Average points]
>     SparkMap[Average points] => RddChannel => SparkMap[Tag stable centroids]
>     SparkMap[Tag stable centroids] => RddChannel => SparkCache[convert out@SparkMap[Tag stable centroids]]
> Out SparkCache[convert out@SparkMap[Tag stable centroids]] => RddChannel
> Out SparkCache[convert out@SparkMap[Tag stable centroids]] => RddChannel

>>> ExecutionStage[T[SparkMap[1+1->1, id=401217ef]]]:
> In  RddChannel => SparkMap[1+1->1, id=401217ef]
> In  BroadcastChannel => SparkMap[1+1->1, id=401217ef]
>     SparkMap[1+1->1, id=401217ef] => RddChannel => SparkFilter[1->1, id=5701b279]
>     SparkFilter[1->1, id=5701b279] => RddChannel => SparkMap[Find nearest centroid]
>     SparkMap[Find nearest centroid] => RddChannel => SparkCache[convert out@SparkMap[Find nearest centroid]]
> Out SparkCache[convert out@SparkMap[Find nearest centroid]] => RddChannel
> Out SparkCache[convert out@SparkMap[Find nearest centroid]] => RddChannel

>>> ExecutionStage[T[SparkFilter[Filter stable centroids]]]:
> In  RddChannel => SparkFilter[Filter stable centroids]
> Out SparkFilter[Filter stable centroids] => RddChannel

>>> ExecutionStage[T[SparkFilter[Filter unstable centroids]]]:
> In  RddChannel => SparkFilter[Filter unstable centroids]
>     SparkFilter[Filter unstable centroids] => RddChannel => SparkCollect[convert out@SparkFilter[Filter unstable centroids]]
> Out SparkCollect[convert out@SparkFilter[Filter unstable centroids]] => CollectionChannel

>>> ExecutionStage[T[SparkUnionAll[2->1, id=77096cd7]]]:
> In  RddChannel => SparkUnionAll[2->1, id=77096cd7]
> In  RddChannel => SparkUnionAll[2->1, id=77096cd7]
> Out SparkUnionAll[2->1, id=77096cd7] => RddChannel

>>> ExecutionStage[T[SparkFilter[Filter stable centroids]]]:
> In  RddChannel => SparkFilter[Filter stable centroids]
> Out SparkFilter[Filter stable centroids] => RddChannel

>>> ExecutionStage[T[SparkFilter[Filter unstable centroids]]]:
> In  RddChannel => SparkFilter[Filter unstable centroids]
>     SparkFilter[Filter unstable centroids] => RddChannel => SparkCollect[convert out@SparkFilter[Filter unstable centroids]]
> Out SparkCollect[convert out@SparkFilter[Filter unstable centroids]] => CollectionChannel

>>> ExecutionStage[T[SparkMap[1+1->1, id=17c658fc]]]:
> In  RddChannel => SparkMap[1+1->1, id=17c658fc]
> In  BroadcastChannel => SparkMap[1+1->1, id=17c658fc]
>     SparkMap[1+1->1, id=17c658fc] => RddChannel => SparkFilter[1->1, id=31fe1e99]
>     SparkFilter[1->1, id=31fe1e99] => RddChannel => SparkMap[Find nearest centroid]
>     SparkMap[Find nearest centroid] => RddChannel => SparkCache[convert out@SparkMap[Find nearest centroid]]
> Out SparkCache[convert out@SparkMap[Find nearest centroid]] => RddChannel
> Out SparkCache[convert out@SparkMap[Find nearest centroid]] => RddChannel

>>> ExecutionStage[T[SparkReduceBy[Add up points]]]:
> In  RddChannel => SparkReduceBy[Add up points]
>     SparkReduceBy[Add up points] => RddChannel => SparkMap[Average points]
>     SparkMap[Average points] => RddChannel => SparkMap[Tag stable centroids]
>     SparkMap[Tag stable centroids] => RddChannel => SparkCache[convert out@SparkMap[Tag stable centroids]]
> Out SparkCache[convert out@SparkMap[Tag stable centroids]] => RddChannel
> Out SparkCache[convert out@SparkMap[Tag stable centroids]] => RddChannel

>>> ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]]:
> In  CollectionChannel => SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel

>>> ExecutionStage[T[SparkUnionAll[2->1, id=505509d0]]]:
> In  RddChannel => SparkUnionAll[2->1, id=505509d0]
> In  RddChannel => SparkUnionAll[2->1, id=505509d0]
> Out SparkUnionAll[2->1, id=505509d0] => RddChannel

>>> ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]]:
> In  CollectionChannel => SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel

>>> ExecutionStage[T[SparkReduceBy[Add up points]]]:
> In  RddChannel => SparkReduceBy[Add up points]
>     SparkReduceBy[Add up points] => RddChannel => SparkMap[Average points]
>     SparkMap[Average points] => RddChannel => SparkMap[Tag stable centroids]
>     SparkMap[Tag stable centroids] => RddChannel => SparkCache[convert out@SparkMap[Tag stable centroids]]
> Out SparkCache[convert out@SparkMap[Tag stable centroids]] => RddChannel
> Out SparkCache[convert out@SparkMap[Tag stable centroids]] => RddChannel

>>> ExecutionStage[T[SparkMap[1+1->1, id=7e17326d]]]:
> In  RddChannel => SparkMap[1+1->1, id=7e17326d]
> In  BroadcastChannel => SparkMap[1+1->1, id=7e17326d]
>     SparkMap[1+1->1, id=7e17326d] => RddChannel => SparkFilter[1->1, id=39012a93]
>     SparkFilter[1->1, id=39012a93] => RddChannel => SparkMap[Find nearest centroid]
>     SparkMap[Find nearest centroid] => RddChannel => SparkReduceBy[Add up points]
>     SparkReduceBy[Add up points] => RddChannel => SparkMap[Average points]
>     SparkMap[Average points] => RddChannel => SparkMap[Tag stable centroids]
>     SparkMap[Tag stable centroids] => RddChannel => SparkFilter[Filter stable centroids]
> Out SparkFilter[Filter stable centroids] => RddChannel

>>> ExecutionStage[T[SparkFilter[Filter stable centroids]]]:
> In  RddChannel => SparkFilter[Filter stable centroids]
> Out SparkFilter[Filter stable centroids] => RddChannel

>>> ExecutionStage[T[SparkFilter[Filter unstable centroids]]]:
> In  RddChannel => SparkFilter[Filter unstable centroids]
>     SparkFilter[Filter unstable centroids] => RddChannel => SparkCollect[convert out@SparkFilter[Filter unstable centroids]]
> Out SparkCollect[convert out@SparkFilter[Filter unstable centroids]] => CollectionChannel

>>> ExecutionStage[T[SparkUnionAll[2->1, id=127ead93]]]:
> In  RddChannel => SparkUnionAll[2->1, id=127ead93]
> In  RddChannel => SparkUnionAll[2->1, id=127ead93]
> Out SparkUnionAll[2->1, id=127ead93] => RddChannel

>>> ExecutionStage[T[SparkFilter[Filter unstable centroids]]]:
> In  RddChannel => SparkFilter[Filter unstable centroids]
>     SparkFilter[Filter unstable centroids] => RddChannel => SparkCollect[convert out@SparkFilter[Filter unstable centroids]]
> Out SparkCollect[convert out@SparkFilter[Filter unstable centroids]] => CollectionChannel

>>> ExecutionStage[T[SparkFilter[Filter stable centroids]]]:
> In  RddChannel => SparkFilter[Filter stable centroids]
> Out SparkFilter[Filter stable centroids] => RddChannel

>>> ExecutionStage[T[SparkUnionAll[2->1, id=6e725eec]]]:
> In  RddChannel => SparkUnionAll[2->1, id=6e725eec]
> In  RddChannel => SparkUnionAll[2->1, id=6e725eec]
> Out SparkUnionAll[2->1, id=6e725eec] => RddChannel

>>> ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]]:
> In  CollectionChannel => SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel

>>> ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]]:
> In  CollectionChannel => SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel

>>> ExecutionStage[T[SparkCount[1->1, id=7329afc6]]]:
> In  RddChannel => SparkCount[1->1, id=7329afc6]
> Out SparkCount[1->1, id=7329afc6] => CollectionChannel

>>> ExecutionStage[T[SparkCollectionSource[convert out@SparkCount[1->1, id=7329afc6]]]]:
> In  CollectionChannel => SparkCollectionSource[convert out@SparkCount[1->1, id=7329afc6]]
>     SparkCollectionSource[convert out@SparkCount[1->1, id=7329afc6]] => RddChannel => SparkLocalCallbackSink[collect()]


[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkCollectionSource[Load random centroids]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkTextFileSource[Read file]]].
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Running Spark version 1.6.2
[kmeansUnrolled.main()] INFO org.apache.spark.SecurityManager - Changing view acls to: jonas.kemper
[kmeansUnrolled.main()] INFO org.apache.spark.SecurityManager - Changing modify acls to: jonas.kemper
[kmeansUnrolled.main()] INFO org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jonas.kemper); users with modify permissions: Set(jonas.kemper)
[kmeansUnrolled.main()] INFO org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43801.
[sparkDriverActorSystem-akka.actor.default-dispatcher-3] INFO akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[sparkDriverActorSystem-akka.actor.default-dispatcher-3] INFO Remoting - Starting remoting
[sparkDriverActorSystem-akka.actor.default-dispatcher-2] INFO Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.16.21.111:38079]
[kmeansUnrolled.main()] INFO org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 38079.
[kmeansUnrolled.main()] INFO org.apache.spark.SparkEnv - Registering MapOutputTracker
[kmeansUnrolled.main()] INFO org.apache.spark.SparkEnv - Registering BlockManagerMaster
[kmeansUnrolled.main()] INFO org.apache.spark.storage.DiskBlockManager - Created local directory at /data/tmp/blockmgr-63966d22-bb00-4b64-b7d8-7b1410e5cc07
[kmeansUnrolled.main()] INFO org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1104.4 MB
[kmeansUnrolled.main()] INFO org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[kmeansUnrolled.main()] INFO org.spark-project.jetty.server.Server - jetty-8.y.z-SNAPSHOT
[kmeansUnrolled.main()] INFO org.spark-project.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
[kmeansUnrolled.main()] INFO org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[kmeansUnrolled.main()] INFO org.apache.spark.ui.SparkUI - Started SparkUI at http://172.16.21.111:4040
[kmeansUnrolled.main()] INFO org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[kmeansUnrolled.main()] INFO org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39672.
[kmeansUnrolled.main()] INFO org.apache.spark.network.netty.NettyBlockTransferService - Server created on 39672
[kmeansUnrolled.main()] INFO org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[dispatcher-event-loop-2] INFO org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:39672 with 1104.4 MB RAM, BlockManagerId(driver, localhost, 39672)
[kmeansUnrolled.main()] INFO org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkCollectionSource[Load random centroids]]]:
>     SparkCollectionSource[Load random centroids] => RddChannel => SparkCollect[convert output@SparkCollectionSource[Load random centroids]]
> Out SparkCollect[convert output@SparkCollectionSource[Load random centroids]] => CollectionChannel
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - Execution of T[SparkCollectionSource[Load random centroids]] took suspiciously long (0:00:00.652).
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: collect at SparkCollectOperator.java:43
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 0 (collect at SparkCollectOperator.java:43) with 4 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (collect at SparkCollectOperator.java:43)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 3.2 KB, free 3.2 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1962.0 B, free 5.2 KB)
[dispatcher-event-loop-4] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:39672 (size: 1962.0 B, free: 1104.4 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 3375 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2294 bytes result sent to driver
[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1,PROCESS_LOCAL, 3375 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 127 ms on localhost (1/4)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 2294 bytes result sent to driver
[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2,PROCESS_LOCAL, 3375 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 27 ms on localhost (2/4)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 2294 bytes result sent to driver
[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3,PROCESS_LOCAL, 3375 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 27 ms on localhost (3/4)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 2294 bytes result sent to driver
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 24 ms on localhost (4/4)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (collect at SparkCollectOperator.java:43) finished in 0.213 s
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 0 finished: collect at SparkCollectOperator.java:43, took 0.916644 s
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 2 items in 0:00:01.012 (estimated (0:00:04.525 .. 0:00:04.525, p=90.00%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkCollectionSource[Load random centroids]]] in 0:00:01.689 (1689 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkBroadcast[convert output@SparkCollectionSource[Load random centroids]]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkTextFileSource[Read file]]]:
>     SparkTextFileSource[Read file] => RddChannel => SparkMap[Create points]
> Out SparkMap[Create points] => RddChannel
[kmeansUnrolled.main()] INFO org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 127.1 KB, free 132.2 KB)
[kmeansUnrolled.main()] INFO org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 13.8 KB, free 146.0 KB)
[dispatcher-event-loop-6] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:39672 (size: 13.8 KB, free: 1104.4 MB)
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Created broadcast 1 from textFile at SparkTextFileSource.java:52
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 1 items in 0:00:00.151 (estimated (0:00:00.099 .. 0:00:00.099, p=85.50%)).
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - Execution of T[SparkMap[Create points]] took suspiciously long (0:00:00.013).
[kmeansUnrolled.main()] INFO org.qcri.rheem.spark.execution.SparkExecutor - T[SparkMap[Create points]] was not executed eagerly as requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkTextFileSource[Read file]]] in 0:00:00.165 (165 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkBroadcast[convert output@SparkCollectionSource[Load random centroids]]]]:
> In  CollectionChannel => SparkBroadcast[convert output@SparkCollectionSource[Load random centroids]]
> Out SparkBroadcast[convert output@SparkCollectionSource[Load random centroids]] => BroadcastChannel
> Out SparkBroadcast[convert output@SparkCollectionSource[Load random centroids]] => BroadcastChannel
[kmeansUnrolled.main()] INFO org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 5.7 KB, free 151.7 KB)
[kmeansUnrolled.main()] INFO org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.3 KB, free 155.0 KB)
[dispatcher-event-loop-7] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:39672 (size: 3.3 KB, free: 1104.4 MB)
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Created broadcast 2 from broadcast at SparkBroadcastOperator.java:46
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - No cardinality available for BroadcastChannel[T[SparkBroadcast[convert output@SparkCollectionSource[Load random centroids]]]->[T[SparkMap[Tag stable centroids - iteration zero]], T[SparkMap[Find nearest centroid - iteration zero]]]], although it was requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 1 items in 0:00:00.037 (estimated (0:00:00.015 .. 0:00:00.015, p=90.00%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkBroadcast[convert output@SparkCollectionSource[Load random centroids]]]] in 0:00:00.040 (40 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkMap[Find nearest centroid - iteration zero]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkMap[Find nearest centroid - iteration zero]]]:
> In  RddChannel => SparkMap[Find nearest centroid - iteration zero]
> In  BroadcastChannel => SparkMap[Find nearest centroid - iteration zero]
>     SparkMap[Find nearest centroid - iteration zero] => RddChannel => SparkCache[convert out@SparkMap[Find nearest centroid - iteration zero]]
> Out SparkCache[convert out@SparkMap[Find nearest centroid - iteration zero]] => RddChannel
> Out SparkCache[convert out@SparkMap[Find nearest centroid - iteration zero]] => RddChannel
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - Execution of T[SparkMap[Find nearest centroid - iteration zero]] took suspiciously long (0:00:00.013).
[kmeansUnrolled.main()] INFO org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: foreachPartition at SparkCacheOperator.java:44
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 1 (foreachPartition at SparkCacheOperator.java:44) with 1 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (foreachPartition at SparkCacheOperator.java:44)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[7] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 6.5 KB, free 161.6 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.6 KB, free 165.2 KB)
[dispatcher-event-loop-1] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:39672 (size: 3.6 KB, free: 1104.4 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, partition 0,ANY, 2152 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
[Executor task launch worker-0] INFO org.apache.spark.CacheManager - Partition rdd_7_0 not found, computing it
[Executor task launch worker-0] INFO org.apache.spark.rdd.HadoopRDD - Input split: hdfs://tenemhead2/data/2dpoints/tmp_kmeans_big.txt:0+14770
[Executor task launch worker-0] INFO org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[Executor task launch worker-0] INFO org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[Executor task launch worker-0] INFO org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[Executor task launch worker-0] INFO org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[Executor task launch worker-0] INFO org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[Executor task launch worker-0] INFO org.apache.spark.storage.MemoryStore - Block rdd_7_0 stored as values in memory (estimated size 46.1 KB, free 211.3 KB)
[dispatcher-event-loop-5] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_7_0 in memory on localhost:39672 (size: 46.1 KB, free: 1104.3 MB)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 2664 bytes result sent to driver
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 381 ms on localhost (1/1)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (foreachPartition at SparkCacheOperator.java:44) finished in 0.383 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 1 finished: foreachPartition at SparkCacheOperator.java:44, took 0.414556 s
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 4 items in 0:00:00.488 (estimated (0:00:00.060 .. 0:00:00.060, p=18.05%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkMap[Find nearest centroid - iteration zero]]] in 0:00:00.505 (505 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkReduceBy[Add up points - iteration zero]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkReduceBy[Add up points - iteration zero]]]:
> In  RddChannel => SparkReduceBy[Add up points - iteration zero]
>     SparkReduceBy[Add up points - iteration zero] => RddChannel => SparkMap[Average points - iteration zero]
>     SparkMap[Average points - iteration zero] => RddChannel => SparkMap[Tag stable centroids - iteration zero]
>     SparkMap[Tag stable centroids - iteration zero] => RddChannel => SparkCache[convert out@SparkMap[Tag stable centroids - iteration zero]]
> Out SparkCache[convert out@SparkMap[Tag stable centroids - iteration zero]] => RddChannel
> Out SparkCache[convert out@SparkMap[Tag stable centroids - iteration zero]] => RddChannel
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - Execution of T[SparkReduceBy[Add up points - iteration zero]] took suspiciously long (0:00:00.061).
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: foreachPartition at SparkCacheOperator.java:44
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 8 (mapToPair at SparkReduceByOperator.java:73)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 2 (foreachPartition at SparkCacheOperator.java:44) with 4 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (foreachPartition at SparkCacheOperator.java:44)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (Add up points - iteration zero MapPartitionsRDD[8] at mapToPair at SparkReduceByOperator.java:73), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 7.5 KB, free 218.9 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.1 KB, free 223.0 KB)
[dispatcher-event-loop-1] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:39672 (size: 4.1 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (Add up points - iteration zero MapPartitionsRDD[8] at mapToPair at SparkReduceByOperator.java:73)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 2141 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 5)
[Executor task launch worker-0] INFO org.apache.spark.storage.BlockManager - Found block rdd_7_0 locally
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 5). 2291 bytes result sent to driver
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 5) in 198 ms on localhost (1/1)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (mapToPair at SparkReduceByOperator.java:73) finished in 0.203 s
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[13] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 6.9 KB, free 229.9 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.7 KB, free 233.6 KB)
[dispatcher-event-loop-6] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:39672 (size: 3.7 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 4 tasks
[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 6, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 6)
[Executor task launch worker-0] INFO org.apache.spark.CacheManager - Partition rdd_13_0 not found, computing it
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 10 ms
[Executor task launch worker-0] INFO org.apache.spark.storage.MemoryStore - Block rdd_13_0 stored as values in memory (estimated size 1352.0 B, free 234.9 KB)
[dispatcher-event-loop-3] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_13_0 in memory on localhost:39672 (size: 1352.0 B, free: 1104.3 MB)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 6). 1805 bytes result sent to driver
[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 7, localhost, partition 1,NODE_LOCAL, 1894 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 7)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 6) in 89 ms on localhost (1/4)
[Executor task launch worker-0] INFO org.apache.spark.CacheManager - Partition rdd_13_1 not found, computing it
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-0] INFO org.apache.spark.storage.MemoryStore - Block rdd_13_1 stored as values in memory (estimated size 1296.0 B, free 236.2 KB)
[dispatcher-event-loop-6] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_13_1 in memory on localhost:39672 (size: 1296.0 B, free: 1104.3 MB)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 7). 1805 bytes result sent to driver
[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 8, localhost, partition 2,NODE_LOCAL, 1894 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 8)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 7) in 22 ms on localhost (2/4)
[Executor task launch worker-0] INFO org.apache.spark.CacheManager - Partition rdd_13_2 not found, computing it
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-0] INFO org.apache.spark.storage.MemoryStore - Block rdd_13_2 stored as values in memory (estimated size 1352.0 B, free 237.5 KB)
[dispatcher-event-loop-3] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_13_2 in memory on localhost:39672 (size: 1352.0 B, free: 1104.3 MB)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 8). 1805 bytes result sent to driver
[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 9, localhost, partition 3,NODE_LOCAL, 1894 bytes)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 8) in 20 ms on localhost (3/4)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 3.0 in stage 3.0 (TID 9)
[Executor task launch worker-0] INFO org.apache.spark.CacheManager - Partition rdd_13_3 not found, computing it
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-0] INFO org.apache.spark.storage.MemoryStore - Block rdd_13_3 stored as values in memory (estimated size 1352.0 B, free 238.8 KB)
[dispatcher-event-loop-6] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_13_3 in memory on localhost:39672 (size: 1352.0 B, free: 1104.3 MB)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 3.0 in stage 3.0 (TID 9). 1805 bytes result sent to driver
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 9) in 16 ms on localhost (4/4)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (foreachPartition at SparkCacheOperator.java:44) finished in 0.146 s
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 2 finished: foreachPartition at SparkCacheOperator.java:44, took 0.434464 s
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - No cardinality available for RddChannel[T[SparkCache[convert out@SparkMap[Find nearest centroid - iteration zero]]]->[T[SparkMap[1+1->1, id=5e7550b5]], T[SparkReduceBy[Add up points - iteration zero]]]], although it was requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 4 items in 0:00:00.448 (estimated (0:00:00.089 .. 0:00:00.089, p=9.50%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkReduceBy[Add up points - iteration zero]]] in 0:00:00.530 (530 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkFilter[Filter stable centroids - iteration zero]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkFilter[Filter unstable centroids - iteration zero]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkFilter[Filter stable centroids - iteration zero]]]:
> In  RddChannel => SparkFilter[Filter stable centroids - iteration zero]
> Out SparkFilter[Filter stable centroids - iteration zero] => RddChannel
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - Execution of T[SparkFilter[Filter stable centroids - iteration zero]] took suspiciously long (0:00:00.013).
[kmeansUnrolled.main()] INFO org.qcri.rheem.spark.execution.SparkExecutor - T[SparkFilter[Filter stable centroids - iteration zero]] was not executed eagerly as requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkFilter[Filter stable centroids - iteration zero]]] in 0:00:00.013 (13 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkFilter[Filter unstable centroids - iteration zero]]]:
> In  RddChannel => SparkFilter[Filter unstable centroids - iteration zero]
>     SparkFilter[Filter unstable centroids - iteration zero] => RddChannel => SparkCollect[convert out@SparkFilter[Filter unstable centroids - iteration zero]]
> Out SparkCollect[convert out@SparkFilter[Filter unstable centroids - iteration zero]] => CollectionChannel
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: collect at SparkCollectOperator.java:43
[dag-scheduler-event-loop] INFO org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 145 bytes
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 3 (collect at SparkCollectOperator.java:43) with 4 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (collect at SparkCollectOperator.java:43)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[17] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 7.0 KB, free 245.9 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.8 KB, free 249.6 KB)
[dispatcher-event-loop-1] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:39672 (size: 3.8 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 4 tasks
[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 10, localhost, partition 0,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 10)
[Executor task launch worker-0] INFO org.apache.spark.storage.BlockManager - Found block rdd_13_0 locally
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 10). 3280 bytes result sent to driver
[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 5.0 (TID 11, localhost, partition 1,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 5.0 (TID 11)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 10) in 9 ms on localhost (1/4)
[Executor task launch worker-0] INFO org.apache.spark.storage.BlockManager - Found block rdd_13_1 locally
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 5.0 (TID 11). 3323 bytes result sent to driver
[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 5.0 (TID 12, localhost, partition 2,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 2.0 in stage 5.0 (TID 12)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 5.0 (TID 11) in 8 ms on localhost (2/4)
[Executor task launch worker-0] INFO org.apache.spark.storage.BlockManager - Found block rdd_13_2 locally
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 2.0 in stage 5.0 (TID 12). 3240 bytes result sent to driver
[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 5.0 (TID 13, localhost, partition 3,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 3.0 in stage 5.0 (TID 13)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 5.0 (TID 12) in 8 ms on localhost (3/4)
[Executor task launch worker-0] INFO org.apache.spark.storage.BlockManager - Found block rdd_13_3 locally
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 3.0 in stage 5.0 (TID 13). 3323 bytes result sent to driver
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 5.0 (TID 13) in 7 ms on localhost (4/4)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (collect at SparkCollectOperator.java:43) finished in 0.030 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 3 finished: collect at SparkCollectOperator.java:43, took 0.054423 s
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - No cardinality available for RddChannel[T[SparkCache[convert out@SparkMap[Tag stable centroids - iteration zero]]]->[T[SparkFilter[Filter unstable centroids - iteration zero]], T[SparkFilter[Filter stable centroids - iteration zero]]]], although it was requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 2 items in 0:00:00.067 (estimated (0:00:00.035 .. 0:00:00.035, p=18.00%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkFilter[Filter unstable centroids - iteration zero]]] in 0:00:00.076 (76 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids - iteration zero]]]].
[kmeansUnrolled.main()] INFO org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 13 from persistence list
[block-manager-slave-async-thread-pool-0] INFO org.apache.spark.storage.BlockManager - Removing RDD 13
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids - iteration zero]]]]:
> In  CollectionChannel => SparkBroadcast[convert out@SparkFilter[Filter unstable centroids - iteration zero]]
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids - iteration zero]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids - iteration zero]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids - iteration zero]] => BroadcastChannel
[kmeansUnrolled.main()] INFO org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 5.1 KB, free 249.5 KB)
[kmeansUnrolled.main()] INFO org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.2 KB, free 252.7 KB)
[dispatcher-event-loop-5] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:39672 (size: 3.2 KB, free: 1104.3 MB)
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Created broadcast 7 from broadcast at SparkBroadcastOperator.java:46
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - No cardinality available for BroadcastChannel[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids - iteration zero]]]->[T[SparkMap[1+1->1, id=5e7550b5]], T[SparkMap[Tag stable centroids]], T[SparkMap[Find nearest centroid]]]], although it was requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 1 items in 0:00:00.005 (estimated (0:00:00.015 .. 0:00:00.015, p=72.90%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids - iteration zero]]]] in 0:00:00.006 (6 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkMap[1+1->1, id=5e7550b5]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkMap[1+1->1, id=5e7550b5]]]:
> In  RddChannel => SparkMap[1+1->1, id=5e7550b5]
> In  BroadcastChannel => SparkMap[1+1->1, id=5e7550b5]
>     SparkMap[1+1->1, id=5e7550b5] => RddChannel => SparkFilter[1->1, id=5b7781d]
>     SparkFilter[1->1, id=5b7781d] => RddChannel => SparkMap[Find nearest centroid]
>     SparkMap[Find nearest centroid] => RddChannel => SparkCache[convert out@SparkMap[Find nearest centroid]]
> Out SparkCache[convert out@SparkMap[Find nearest centroid]] => RddChannel
> Out SparkCache[convert out@SparkMap[Find nearest centroid]] => RddChannel
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: foreachPartition at SparkCacheOperator.java:44
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 4 (foreachPartition at SparkCacheOperator.java:44) with 1 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (foreachPartition at SparkCacheOperator.java:44)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[21] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 7.4 KB, free 260.1 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.9 KB, free 264.0 KB)
[dispatcher-event-loop-0] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on localhost:39672 (size: 3.9 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[21] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks
[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 14, localhost, partition 0,PROCESS_LOCAL, 2152 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 14)
[Executor task launch worker-0] INFO org.apache.spark.CacheManager - Partition rdd_21_0 not found, computing it
[Executor task launch worker-0] INFO org.apache.spark.storage.BlockManager - Found block rdd_7_0 locally
[Executor task launch worker-0] INFO org.apache.spark.storage.MemoryStore - Block rdd_21_0 stored as values in memory (estimated size 41.0 KB, free 305.0 KB)
[dispatcher-event-loop-4] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_21_0 in memory on localhost:39672 (size: 41.0 KB, free: 1104.2 MB)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 14). 2679 bytes result sent to driver
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 14) in 78 ms on localhost (1/1)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (foreachPartition at SparkCacheOperator.java:44) finished in 0.078 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 4 finished: foreachPartition at SparkCacheOperator.java:44, took 0.089324 s
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 4 items in 0:00:00.096 (estimated (0:00:00.070 .. 0:00:00.070, p=11.81%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkMap[1+1->1, id=5e7550b5]]] in 0:00:00.111 (111 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkReduceBy[Add up points]]].
[kmeansUnrolled.main()] INFO org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 7 from persistence list
[block-manager-slave-async-thread-pool-2] INFO org.apache.spark.storage.BlockManager - Removing RDD 7
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkReduceBy[Add up points]]]:
> In  RddChannel => SparkReduceBy[Add up points]
>     SparkReduceBy[Add up points] => RddChannel => SparkMap[Average points]
>     SparkMap[Average points] => RddChannel => SparkMap[Tag stable centroids]
>     SparkMap[Tag stable centroids] => RddChannel => SparkCache[convert out@SparkMap[Tag stable centroids]]
> Out SparkCache[convert out@SparkMap[Tag stable centroids]] => RddChannel
> Out SparkCache[convert out@SparkMap[Tag stable centroids]] => RddChannel
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - Execution of T[SparkReduceBy[Add up points]] took suspiciously long (0:00:00.015).
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: foreachPartition at SparkCacheOperator.java:44
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 22 (mapToPair at SparkReduceByOperator.java:73)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 5 (foreachPartition at SparkCacheOperator.java:44) with 4 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (foreachPartition at SparkCacheOperator.java:44)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 7)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 7)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 7 (Add up points MapPartitionsRDD[22] at mapToPair at SparkReduceByOperator.java:73), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 8.4 KB, free 267.3 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.4 KB, free 271.7 KB)
[dispatcher-event-loop-3] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on localhost:39672 (size: 4.4 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 7 (Add up points MapPartitionsRDD[22] at mapToPair at SparkReduceByOperator.java:73)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks
[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 15, localhost, partition 0,PROCESS_LOCAL, 2141 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 15)
[Executor task launch worker-0] INFO org.apache.spark.storage.BlockManager - Found block rdd_21_0 locally
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 15). 2306 bytes result sent to driver
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 15) in 30 ms on localhost (1/1)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 7 (mapToPair at SparkReduceByOperator.java:73) finished in 0.032 s
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 8)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[27] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 6.9 KB, free 278.6 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.7 KB, free 282.3 KB)
[dispatcher-event-loop-6] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on localhost:39672 (size: 3.7 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 8 (MapPartitionsRDD[27] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 4 tasks
[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 16, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 16)
[Executor task launch worker-0] INFO org.apache.spark.CacheManager - Partition rdd_27_0 not found, computing it
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-0] INFO org.apache.spark.storage.MemoryStore - Block rdd_27_0 stored as values in memory (estimated size 1192.0 B, free 283.5 KB)
[dispatcher-event-loop-2] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_27_0 in memory on localhost:39672 (size: 1192.0 B, free: 1104.3 MB)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 16). 1805 bytes result sent to driver
[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 8.0 (TID 17, localhost, partition 1,NODE_LOCAL, 1894 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 8.0 (TID 17)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 16) in 15 ms on localhost (1/4)
[Executor task launch worker-0] INFO org.apache.spark.CacheManager - Partition rdd_27_1 not found, computing it
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-0] INFO org.apache.spark.storage.MemoryStore - Block rdd_27_1 stored as values in memory (estimated size 1248.0 B, free 284.7 KB)
[dispatcher-event-loop-6] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_27_1 in memory on localhost:39672 (size: 1248.0 B, free: 1104.3 MB)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 8.0 (TID 17). 1805 bytes result sent to driver
[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 8.0 (TID 18, localhost, partition 2,NODE_LOCAL, 1894 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 2.0 in stage 8.0 (TID 18)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 8.0 (TID 17) in 13 ms on localhost (2/4)
[Executor task launch worker-0] INFO org.apache.spark.CacheManager - Partition rdd_27_2 not found, computing it
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-0] INFO org.apache.spark.storage.MemoryStore - Block rdd_27_2 stored as values in memory (estimated size 1144.0 B, free 285.8 KB)
[dispatcher-event-loop-2] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_27_2 in memory on localhost:39672 (size: 1144.0 B, free: 1104.3 MB)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 2.0 in stage 8.0 (TID 18). 1805 bytes result sent to driver
[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 8.0 (TID 19, localhost, partition 3,NODE_LOCAL, 1894 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 3.0 in stage 8.0 (TID 19)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 8.0 (TID 18) in 12 ms on localhost (3/4)
[Executor task launch worker-0] INFO org.apache.spark.CacheManager - Partition rdd_27_3 not found, computing it
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-0] INFO org.apache.spark.storage.MemoryStore - Block rdd_27_3 stored as values in memory (estimated size 1248.0 B, free 287.0 KB)
[dispatcher-event-loop-6] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_27_3 in memory on localhost:39672 (size: 1248.0 B, free: 1104.3 MB)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 3.0 in stage 8.0 (TID 19). 1805 bytes result sent to driver
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 8.0 (TID 19) in 13 ms on localhost (4/4)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (foreachPartition at SparkCacheOperator.java:44) finished in 0.052 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 5 finished: foreachPartition at SparkCacheOperator.java:44, took 0.107159 s
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - No cardinality available for RddChannel[T[SparkCache[convert out@SparkMap[Find nearest centroid]]]->[T[SparkMap[1+1->1, id=757832fd]], T[SparkReduceBy[Add up points]]]], although it was requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 4 items in 0:00:00.114 (estimated (0:00:00.090 .. 0:00:00.090, p=8.10%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkReduceBy[Add up points]]] in 0:00:00.139 (139 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkFilter[Filter unstable centroids]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkFilter[Filter stable centroids]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkFilter[Filter unstable centroids]]]:
> In  RddChannel => SparkFilter[Filter unstable centroids]
>     SparkFilter[Filter unstable centroids] => RddChannel => SparkCollect[convert out@SparkFilter[Filter unstable centroids]]
> Out SparkCollect[convert out@SparkFilter[Filter unstable centroids]] => CollectionChannel
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: collect at SparkCollectOperator.java:43
[dag-scheduler-event-loop] INFO org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 147 bytes
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 6 (collect at SparkCollectOperator.java:43) with 4 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 10 (collect at SparkCollectOperator.java:43)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 9)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 10 (MapPartitionsRDD[29] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 7.1 KB, free 294.1 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.8 KB, free 297.9 KB)
[dispatcher-event-loop-3] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on localhost:39672 (size: 3.8 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 10 (MapPartitionsRDD[29] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 4 tasks
[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 20, localhost, partition 0,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 20)
[Executor task launch worker-0] INFO org.apache.spark.storage.BlockManager - Found block rdd_27_0 locally
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 10.0 (TID 20). 2838 bytes result sent to driver
[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 10.0 (TID 21, localhost, partition 1,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 10.0 (TID 21)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 10.0 (TID 20) in 8 ms on localhost (1/4)
[Executor task launch worker-0] INFO org.apache.spark.storage.BlockManager - Found block rdd_27_1 locally
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 10.0 (TID 21). 2758 bytes result sent to driver
[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 10.0 (TID 22, localhost, partition 2,PROCESS_LOCAL, 1894 bytes)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 10.0 (TID 21) in 9 ms on localhost (2/4)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 2.0 in stage 10.0 (TID 22)
[Executor task launch worker-0] INFO org.apache.spark.storage.BlockManager - Found block rdd_27_2 locally
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 2.0 in stage 10.0 (TID 22). 2918 bytes result sent to driver
[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 10.0 (TID 23, localhost, partition 3,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 3.0 in stage 10.0 (TID 23)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 10.0 (TID 22) in 9 ms on localhost (3/4)
[Executor task launch worker-0] INFO org.apache.spark.storage.BlockManager - Found block rdd_27_3 locally
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 3.0 in stage 10.0 (TID 23). 2798 bytes result sent to driver
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 10.0 (TID 23) in 9 ms on localhost (4/4)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 10 (collect at SparkCollectOperator.java:43) finished in 0.032 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 6 finished: collect at SparkCollectOperator.java:43, took 0.047362 s
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - No cardinality available for RddChannel[T[SparkCache[convert out@SparkMap[Tag stable centroids]]]->[T[SparkFilter[Filter stable centroids]], T[SparkFilter[Filter unstable centroids]]]], although it was requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 2 items in 0:00:00.055 (estimated (0:00:00.035 .. 0:00:00.035, p=14.58%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkFilter[Filter unstable centroids]]] in 0:00:00.063 (63 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkFilter[Filter stable centroids]]]:
> In  RddChannel => SparkFilter[Filter stable centroids]
> Out SparkFilter[Filter stable centroids] => RddChannel
[kmeansUnrolled.main()] INFO org.qcri.rheem.spark.execution.SparkExecutor - T[SparkFilter[Filter stable centroids]] was not executed eagerly as requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkFilter[Filter stable centroids]]] in 0:00:00.007 (7 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkUnionAll[2->1, id=2ef8007e]]].
[kmeansUnrolled.main()] INFO org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 27 from persistence list
[block-manager-slave-async-thread-pool-1] INFO org.apache.spark.storage.BlockManager - Removing RDD 27
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]]:
> In  CollectionChannel => SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
[kmeansUnrolled.main()] INFO org.apache.spark.storage.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 2.8 KB, free 295.9 KB)
[kmeansUnrolled.main()] INFO org.apache.spark.storage.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.2 KB, free 298.1 KB)
[dispatcher-event-loop-7] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on localhost:39672 (size: 2.2 KB, free: 1104.3 MB)
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Created broadcast 12 from broadcast at SparkBroadcastOperator.java:46
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - No cardinality available for BroadcastChannel[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]->[T[SparkMap[Tag stable centroids]], T[SparkMap[Find nearest centroid]], T[SparkMap[1+1->1, id=757832fd]]]], although it was requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 1 items in 0:00:00.006 (estimated (0:00:00.015 .. 0:00:00.015, p=59.05%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]] in 0:00:00.007 (7 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkMap[1+1->1, id=757832fd]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkUnionAll[2->1, id=2ef8007e]]]:
> In  RddChannel => SparkUnionAll[2->1, id=2ef8007e]
> In  RddChannel => SparkUnionAll[2->1, id=2ef8007e]
> Out SparkUnionAll[2->1, id=2ef8007e] => RddChannel
[kmeansUnrolled.main()] INFO org.qcri.rheem.spark.execution.SparkExecutor - T[SparkUnionAll[2->1, id=2ef8007e]] was not executed eagerly as requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkUnionAll[2->1, id=2ef8007e]]] in 0:00:00.009 (9 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkMap[1+1->1, id=757832fd]]]:
> In  RddChannel => SparkMap[1+1->1, id=757832fd]
> In  BroadcastChannel => SparkMap[1+1->1, id=757832fd]
>     SparkMap[1+1->1, id=757832fd] => RddChannel => SparkFilter[1->1, id=3db9b0a1]
>     SparkFilter[1->1, id=3db9b0a1] => RddChannel => SparkMap[Find nearest centroid]
>     SparkMap[Find nearest centroid] => RddChannel => SparkCache[convert out@SparkMap[Find nearest centroid]]
> Out SparkCache[convert out@SparkMap[Find nearest centroid]] => RddChannel
> Out SparkCache[convert out@SparkMap[Find nearest centroid]] => RddChannel
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: foreachPartition at SparkCacheOperator.java:44
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 7 (foreachPartition at SparkCacheOperator.java:44) with 1 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (foreachPartition at SparkCacheOperator.java:44)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[37] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 8.0 KB, free 306.1 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.1 KB, free 310.2 KB)
[dispatcher-event-loop-1] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on localhost:39672 (size: 4.1 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[37] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks
[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 24, localhost, partition 0,PROCESS_LOCAL, 2152 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 24)
[Executor task launch worker-0] INFO org.apache.spark.CacheManager - Partition rdd_37_0 not found, computing it
[Executor task launch worker-0] INFO org.apache.spark.storage.BlockManager - Found block rdd_21_0 locally
[Executor task launch worker-0] INFO org.apache.spark.storage.MemoryStore - Block rdd_37_0 stored as values in memory (estimated size 20.5 KB, free 330.7 KB)
[dispatcher-event-loop-4] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_37_0 in memory on localhost:39672 (size: 20.5 KB, free: 1104.3 MB)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 24). 2694 bytes result sent to driver
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 24) in 40 ms on localhost (1/1)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (foreachPartition at SparkCacheOperator.java:44) finished in 0.039 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 7 finished: foreachPartition at SparkCacheOperator.java:44, took 0.055675 s
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 4 items in 0:00:00.064 (estimated (0:00:00.070 .. 0:00:00.070, p=8.61%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkMap[1+1->1, id=757832fd]]] in 0:00:00.080 (80 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkReduceBy[Add up points]]].
[kmeansUnrolled.main()] INFO org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 21 from persistence list
[block-manager-slave-async-thread-pool-2] INFO org.apache.spark.storage.BlockManager - Removing RDD 21
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkReduceBy[Add up points]]]:
> In  RddChannel => SparkReduceBy[Add up points]
>     SparkReduceBy[Add up points] => RddChannel => SparkMap[Average points]
>     SparkMap[Average points] => RddChannel => SparkMap[Tag stable centroids]
>     SparkMap[Tag stable centroids] => RddChannel => SparkCache[convert out@SparkMap[Tag stable centroids]]
> Out SparkCache[convert out@SparkMap[Tag stable centroids]] => RddChannel
> Out SparkCache[convert out@SparkMap[Tag stable centroids]] => RddChannel
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - Execution of T[SparkReduceBy[Add up points]] took suspiciously long (0:00:00.013).
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: foreachPartition at SparkCacheOperator.java:44
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 38 (mapToPair at SparkReduceByOperator.java:73)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 8 (foreachPartition at SparkCacheOperator.java:44) with 4 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 13 (foreachPartition at SparkCacheOperator.java:44)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 12)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 12)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 12 (Add up points MapPartitionsRDD[38] at mapToPair at SparkReduceByOperator.java:73), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 9.0 KB, free 298.7 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.5 KB, free 303.2 KB)
[dispatcher-event-loop-2] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on localhost:39672 (size: 4.5 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 14 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 12 (Add up points MapPartitionsRDD[38] at mapToPair at SparkReduceByOperator.java:73)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 1 tasks
[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 25, localhost, partition 0,PROCESS_LOCAL, 2141 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 25)
[Executor task launch worker-0] INFO org.apache.spark.storage.BlockManager - Found block rdd_37_0 locally
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 25). 2321 bytes result sent to driver
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 25) in 24 ms on localhost (1/1)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 12 (mapToPair at SparkReduceByOperator.java:73) finished in 0.024 s
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 13)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 13 (MapPartitionsRDD[43] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 6.9 KB, free 310.1 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.7 KB, free 313.9 KB)
[dispatcher-event-loop-6] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on localhost:39672 (size: 3.7 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 15 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 13 (MapPartitionsRDD[43] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 13.0 with 4 tasks
[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 13.0 (TID 26, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 13.0 (TID 26)
[Executor task launch worker-0] INFO org.apache.spark.CacheManager - Partition rdd_43_0 not found, computing it
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[Executor task launch worker-0] INFO org.apache.spark.storage.MemoryStore - Block rdd_43_0 stored as values in memory (estimated size 624.0 B, free 314.5 KB)
[dispatcher-event-loop-5] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_43_0 in memory on localhost:39672 (size: 624.0 B, free: 1104.3 MB)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 13.0 (TID 26). 1805 bytes result sent to driver
[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 13.0 (TID 27, localhost, partition 1,NODE_LOCAL, 1894 bytes)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 13.0 (TID 27)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 13.0 (TID 26) in 14 ms on localhost (1/4)
[Executor task launch worker-0] INFO org.apache.spark.CacheManager - Partition rdd_43_1 not found, computing it
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-0] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[Executor task launch worker-0] INFO org.apache.spark.storage.MemoryStore - Block rdd_43_1 stored as values in memory (estimated size 520.0 B, free 315.0 KB)
[dispatcher-event-loop-6] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_43_1 in memory on localhost:39672 (size: 520.0 B, free: 1104.3 MB)
[Executor task launch worker-0] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 13.0 (TID 27). 1805 bytes result sent to driver
[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 13.0 (TID 28, localhost, partition 2,NODE_LOCAL, 1894 bytes)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 13.0 (TID 27) in 16 ms on localhost (2/4)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 2.0 in stage 13.0 (TID 28)
[Executor task launch worker-1] INFO org.apache.spark.CacheManager - Partition rdd_43_2 not found, computing it
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[Executor task launch worker-1] INFO org.apache.spark.storage.MemoryStore - Block rdd_43_2 stored as values in memory (estimated size 728.0 B, free 315.7 KB)
[dispatcher-event-loop-5] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_43_2 in memory on localhost:39672 (size: 728.0 B, free: 1104.3 MB)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 2.0 in stage 13.0 (TID 28). 1805 bytes result sent to driver
[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 13.0 (TID 29, localhost, partition 3,NODE_LOCAL, 1894 bytes)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 13.0 (TID 28) in 14 ms on localhost (3/4)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 3.0 in stage 13.0 (TID 29)
[Executor task launch worker-1] INFO org.apache.spark.CacheManager - Partition rdd_43_3 not found, computing it
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[Executor task launch worker-1] INFO org.apache.spark.storage.MemoryStore - Block rdd_43_3 stored as values in memory (estimated size 568.0 B, free 316.2 KB)
[dispatcher-event-loop-6] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_43_3 in memory on localhost:39672 (size: 568.0 B, free: 1104.3 MB)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 3.0 in stage 13.0 (TID 29). 1805 bytes result sent to driver
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 13.0 (TID 29) in 14 ms on localhost (4/4)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 13.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 13 (foreachPartition at SparkCacheOperator.java:44) finished in 0.055 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 8 finished: foreachPartition at SparkCacheOperator.java:44, took 0.107560 s
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - No cardinality available for RddChannel[T[SparkCache[convert out@SparkMap[Find nearest centroid]]]->[T[SparkReduceBy[Add up points]], T[SparkMap[1+1->1, id=401217ef]]]], although it was requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 4 items in 0:00:00.117 (estimated (0:00:00.090 .. 0:00:00.090, p=7.29%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkReduceBy[Add up points]]] in 0:00:00.142 (142 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkFilter[Filter unstable centroids]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkFilter[Filter stable centroids]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkFilter[Filter unstable centroids]]]:
> In  RddChannel => SparkFilter[Filter unstable centroids]
>     SparkFilter[Filter unstable centroids] => RddChannel => SparkCollect[convert out@SparkFilter[Filter unstable centroids]]
> Out SparkCollect[convert out@SparkFilter[Filter unstable centroids]] => CollectionChannel
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: collect at SparkCollectOperator.java:43
[dag-scheduler-event-loop] INFO org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 2 is 147 bytes
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 9 (collect at SparkCollectOperator.java:43) with 4 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 15 (collect at SparkCollectOperator.java:43)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 14)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 15 (MapPartitionsRDD[45] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_16 stored as values in memory (estimated size 7.1 KB, free 323.3 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.8 KB, free 327.1 KB)
[dispatcher-event-loop-2] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_16_piece0 in memory on localhost:39672 (size: 3.8 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 16 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 15 (MapPartitionsRDD[45] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 4 tasks
[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 30, localhost, partition 0,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 30)
[Executor task launch worker-1] INFO org.apache.spark.storage.BlockManager - Found block rdd_43_0 locally
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 30). 2473 bytes result sent to driver
[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 15.0 (TID 31, localhost, partition 1,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 15.0 (TID 31)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 30) in 9 ms on localhost (1/4)
[Executor task launch worker-1] INFO org.apache.spark.storage.BlockManager - Found block rdd_43_1 locally
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 15.0 (TID 31). 2433 bytes result sent to driver
[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 15.0 (TID 32, localhost, partition 2,PROCESS_LOCAL, 1894 bytes)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 15.0 (TID 31) in 9 ms on localhost (2/4)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 2.0 in stage 15.0 (TID 32)
[Executor task launch worker-1] INFO org.apache.spark.storage.BlockManager - Found block rdd_43_2 locally
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 2.0 in stage 15.0 (TID 32). 2558 bytes result sent to driver
[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 15.0 (TID 33, localhost, partition 3,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 3.0 in stage 15.0 (TID 33)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 15.0 (TID 32) in 10 ms on localhost (3/4)
[Executor task launch worker-1] INFO org.apache.spark.storage.BlockManager - Found block rdd_43_3 locally
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 3.0 in stage 15.0 (TID 33). 2513 bytes result sent to driver
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 15.0 (TID 33) in 8 ms on localhost (4/4)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 15 (collect at SparkCollectOperator.java:43) finished in 0.032 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 9 finished: collect at SparkCollectOperator.java:43, took 0.047337 s
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - No cardinality available for RddChannel[T[SparkCache[convert out@SparkMap[Tag stable centroids]]]->[T[SparkFilter[Filter stable centroids]], T[SparkFilter[Filter unstable centroids]]]], although it was requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 2 items in 0:00:00.072 (estimated (0:00:00.035 .. 0:00:00.035, p=11.81%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkFilter[Filter unstable centroids]]] in 0:00:00.080 (80 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkFilter[Filter stable centroids]]]:
> In  RddChannel => SparkFilter[Filter stable centroids]
> Out SparkFilter[Filter stable centroids] => RddChannel
[kmeansUnrolled.main()] INFO org.qcri.rheem.spark.execution.SparkExecutor - T[SparkFilter[Filter stable centroids]] was not executed eagerly as requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkFilter[Filter stable centroids]]] in 0:00:00.005 (5 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkUnionAll[2->1, id=77096cd7]]].
[kmeansUnrolled.main()] INFO org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 43 from persistence list
[block-manager-slave-async-thread-pool-1] INFO org.apache.spark.storage.BlockManager - Removing RDD 43
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]]:
> In  CollectionChannel => SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
[kmeansUnrolled.main()] INFO org.apache.spark.storage.MemoryStore - Block broadcast_17 stored as values in memory (estimated size 1112.0 B, free 325.8 KB)
[kmeansUnrolled.main()] INFO org.apache.spark.storage.MemoryStore - Block broadcast_17_piece0 stored as bytes in memory (estimated size 1500.0 B, free 327.2 KB)
[dispatcher-event-loop-0] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_17_piece0 in memory on localhost:39672 (size: 1500.0 B, free: 1104.3 MB)
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Created broadcast 17 from broadcast at SparkBroadcastOperator.java:46
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - No cardinality available for BroadcastChannel[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]->[T[SparkMap[Find nearest centroid]], T[SparkMap[1+1->1, id=401217ef]], T[SparkMap[Tag stable centroids]]]], although it was requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 1 items in 0:00:00.005 (estimated (0:00:00.015 .. 0:00:00.015, p=47.83%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]] in 0:00:00.006 (6 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkMap[1+1->1, id=401217ef]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkUnionAll[2->1, id=77096cd7]]]:
> In  RddChannel => SparkUnionAll[2->1, id=77096cd7]
> In  RddChannel => SparkUnionAll[2->1, id=77096cd7]
> Out SparkUnionAll[2->1, id=77096cd7] => RddChannel
[kmeansUnrolled.main()] INFO org.qcri.rheem.spark.execution.SparkExecutor - T[SparkUnionAll[2->1, id=77096cd7]] was not executed eagerly as requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkUnionAll[2->1, id=77096cd7]]] in 0:00:00.004 (4 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkMap[1+1->1, id=401217ef]]]:
> In  RddChannel => SparkMap[1+1->1, id=401217ef]
> In  BroadcastChannel => SparkMap[1+1->1, id=401217ef]
>     SparkMap[1+1->1, id=401217ef] => RddChannel => SparkFilter[1->1, id=5701b279]
>     SparkFilter[1->1, id=5701b279] => RddChannel => SparkMap[Find nearest centroid]
>     SparkMap[Find nearest centroid] => RddChannel => SparkCache[convert out@SparkMap[Find nearest centroid]]
> Out SparkCache[convert out@SparkMap[Find nearest centroid]] => RddChannel
> Out SparkCache[convert out@SparkMap[Find nearest centroid]] => RddChannel
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: foreachPartition at SparkCacheOperator.java:44
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 10 (foreachPartition at SparkCacheOperator.java:44) with 1 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 16 (foreachPartition at SparkCacheOperator.java:44)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 16 (MapPartitionsRDD[53] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_18 stored as values in memory (estimated size 8.5 KB, free 335.8 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_18_piece0 stored as bytes in memory (estimated size 4.1 KB, free 339.9 KB)
[dispatcher-event-loop-3] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_18_piece0 in memory on localhost:39672 (size: 4.1 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 18 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[53] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 16.0 with 1 tasks
[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 16.0 (TID 34, localhost, partition 0,PROCESS_LOCAL, 2152 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 16.0 (TID 34)
[Executor task launch worker-1] INFO org.apache.spark.CacheManager - Partition rdd_53_0 not found, computing it
[Executor task launch worker-1] INFO org.apache.spark.storage.BlockManager - Found block rdd_37_0 locally
[Executor task launch worker-1] INFO org.apache.spark.storage.MemoryStore - Block rdd_53_0 stored as values in memory (estimated size 3.3 KB, free 343.2 KB)
[dispatcher-event-loop-4] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_53_0 in memory on localhost:39672 (size: 3.3 KB, free: 1104.3 MB)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 16.0 (TID 34). 2709 bytes result sent to driver
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 16.0 (TID 34) in 17 ms on localhost (1/1)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 16.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 16 (foreachPartition at SparkCacheOperator.java:44) finished in 0.018 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 10 finished: foreachPartition at SparkCacheOperator.java:44, took 0.031927 s
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 4 items in 0:00:00.037 (estimated (0:00:00.070 .. 0:00:00.070, p=6.28%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkMap[1+1->1, id=401217ef]]] in 0:00:00.048 (48 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkReduceBy[Add up points]]].
[kmeansUnrolled.main()] INFO org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 37 from persistence list
[block-manager-slave-async-thread-pool-2] INFO org.apache.spark.storage.BlockManager - Removing RDD 37
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkReduceBy[Add up points]]]:
> In  RddChannel => SparkReduceBy[Add up points]
>     SparkReduceBy[Add up points] => RddChannel => SparkMap[Average points]
>     SparkMap[Average points] => RddChannel => SparkMap[Tag stable centroids]
>     SparkMap[Tag stable centroids] => RddChannel => SparkCache[convert out@SparkMap[Tag stable centroids]]
> Out SparkCache[convert out@SparkMap[Tag stable centroids]] => RddChannel
> Out SparkCache[convert out@SparkMap[Tag stable centroids]] => RddChannel
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - Execution of T[SparkReduceBy[Add up points]] took suspiciously long (0:00:00.013).
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: foreachPartition at SparkCacheOperator.java:44
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 54 (mapToPair at SparkReduceByOperator.java:73)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 11 (foreachPartition at SparkCacheOperator.java:44) with 4 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 18 (foreachPartition at SparkCacheOperator.java:44)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 17)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 17)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 17 (Add up points MapPartitionsRDD[54] at mapToPair at SparkReduceByOperator.java:73), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_19 stored as values in memory (estimated size 9.6 KB, free 332.3 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.6 KB, free 336.9 KB)
[dispatcher-event-loop-5] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_19_piece0 in memory on localhost:39672 (size: 4.6 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 19 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 17 (Add up points MapPartitionsRDD[54] at mapToPair at SparkReduceByOperator.java:73)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 17.0 with 1 tasks
[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 17.0 (TID 35, localhost, partition 0,PROCESS_LOCAL, 2141 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 17.0 (TID 35)
[Executor task launch worker-1] INFO org.apache.spark.storage.BlockManager - Found block rdd_53_0 locally
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 17.0 (TID 35). 2336 bytes result sent to driver
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 17.0 (TID 35) in 17 ms on localhost (1/1)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 17.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 17 (mapToPair at SparkReduceByOperator.java:73) finished in 0.017 s
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 18)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 18 (MapPartitionsRDD[59] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_20 stored as values in memory (estimated size 6.9 KB, free 343.8 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.7 KB, free 347.5 KB)
[dispatcher-event-loop-6] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_20_piece0 in memory on localhost:39672 (size: 3.7 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 20 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 18 (MapPartitionsRDD[59] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 18.0 with 4 tasks
[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 18.0 (TID 36, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 18.0 (TID 36)
[Executor task launch worker-1] INFO org.apache.spark.CacheManager - Partition rdd_59_0 not found, computing it
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.storage.MemoryStore - Block rdd_59_0 stored as values in memory (estimated size 152.0 B, free 347.7 KB)
[dispatcher-event-loop-7] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_59_0 in memory on localhost:39672 (size: 152.0 B, free: 1104.3 MB)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 18.0 (TID 36). 1805 bytes result sent to driver
[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 18.0 (TID 37, localhost, partition 1,NODE_LOCAL, 1894 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 18.0 (TID 37)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 18.0 (TID 36) in 12 ms on localhost (1/4)
[Executor task launch worker-1] INFO org.apache.spark.CacheManager - Partition rdd_59_1 not found, computing it
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.storage.MemoryStore - Block rdd_59_1 stored as values in memory (estimated size 104.0 B, free 347.8 KB)
[dispatcher-event-loop-6] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_59_1 in memory on localhost:39672 (size: 104.0 B, free: 1104.3 MB)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 18.0 (TID 37). 1805 bytes result sent to driver
[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 18.0 (TID 38, localhost, partition 2,NODE_LOCAL, 1894 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 2.0 in stage 18.0 (TID 38)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 18.0 (TID 37) in 12 ms on localhost (2/4)
[Executor task launch worker-1] INFO org.apache.spark.CacheManager - Partition rdd_59_2 not found, computing it
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.storage.MemoryStore - Block rdd_59_2 stored as values in memory (estimated size 256.0 B, free 348.0 KB)
[dispatcher-event-loop-7] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_59_2 in memory on localhost:39672 (size: 256.0 B, free: 1104.3 MB)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 2.0 in stage 18.0 (TID 38). 1805 bytes result sent to driver
[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 18.0 (TID 39, localhost, partition 3,NODE_LOCAL, 1894 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 3.0 in stage 18.0 (TID 39)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 18.0 (TID 38) in 11 ms on localhost (3/4)
[Executor task launch worker-1] INFO org.apache.spark.CacheManager - Partition rdd_59_3 not found, computing it
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.storage.MemoryStore - Block rdd_59_3 stored as values in memory (estimated size 208.0 B, free 348.2 KB)
[dispatcher-event-loop-6] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_59_3 in memory on localhost:39672 (size: 208.0 B, free: 1104.3 MB)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 3.0 in stage 18.0 (TID 39). 1805 bytes result sent to driver
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 18.0 (TID 39) in 13 ms on localhost (4/4)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 18.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 18 (foreachPartition at SparkCacheOperator.java:44) finished in 0.046 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 11 finished: foreachPartition at SparkCacheOperator.java:44, took 0.085802 s
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - No cardinality available for RddChannel[T[SparkCache[convert out@SparkMap[Find nearest centroid]]]->[T[SparkMap[1+1->1, id=17c658fc]], T[SparkReduceBy[Add up points]]]], although it was requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 4 items in 0:00:00.094 (estimated (0:00:00.080 .. 0:00:00.090, p=6.56%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkReduceBy[Add up points]]] in 0:00:00.120 (120 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkFilter[Filter unstable centroids]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkFilter[Filter stable centroids]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkFilter[Filter unstable centroids]]]:
> In  RddChannel => SparkFilter[Filter unstable centroids]
>     SparkFilter[Filter unstable centroids] => RddChannel => SparkCollect[convert out@SparkFilter[Filter unstable centroids]]
> Out SparkCollect[convert out@SparkFilter[Filter unstable centroids]] => CollectionChannel
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: collect at SparkCollectOperator.java:43
[dag-scheduler-event-loop] INFO org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 3 is 147 bytes
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 12 (collect at SparkCollectOperator.java:43) with 4 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 20 (collect at SparkCollectOperator.java:43)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 19)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 20 (MapPartitionsRDD[61] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_21 stored as values in memory (estimated size 7.1 KB, free 355.3 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_21_piece0 stored as bytes in memory (estimated size 3.8 KB, free 359.0 KB)
[dispatcher-event-loop-5] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_21_piece0 in memory on localhost:39672 (size: 3.8 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 21 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 20 (MapPartitionsRDD[61] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 20.0 with 4 tasks
[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 20.0 (TID 40, localhost, partition 0,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 20.0 (TID 40)
[Executor task launch worker-1] INFO org.apache.spark.storage.BlockManager - Found block rdd_59_0 locally
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 20.0 (TID 40). 2079 bytes result sent to driver
[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 20.0 (TID 41, localhost, partition 1,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 20.0 (TID 41)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 20.0 (TID 40) in 8 ms on localhost (1/4)
[Executor task launch worker-1] INFO org.apache.spark.storage.BlockManager - Found block rdd_59_1 locally
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 20.0 (TID 41). 2079 bytes result sent to driver
[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 20.0 (TID 42, localhost, partition 2,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 2.0 in stage 20.0 (TID 42)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 20.0 (TID 41) in 8 ms on localhost (2/4)
[Executor task launch worker-1] INFO org.apache.spark.storage.BlockManager - Found block rdd_59_2 locally
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 2.0 in stage 20.0 (TID 42). 2079 bytes result sent to driver
[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 20.0 (TID 43, localhost, partition 3,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 3.0 in stage 20.0 (TID 43)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 20.0 (TID 42) in 8 ms on localhost (3/4)
[Executor task launch worker-1] INFO org.apache.spark.storage.BlockManager - Found block rdd_59_3 locally
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 3.0 in stage 20.0 (TID 43). 2079 bytes result sent to driver
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 20.0 (TID 43) in 8 ms on localhost (4/4)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 20.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 20 (collect at SparkCollectOperator.java:43) finished in 0.029 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 12 finished: collect at SparkCollectOperator.java:43, took 0.042971 s
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - No cardinality available for RddChannel[T[SparkCache[convert out@SparkMap[Tag stable centroids]]]->[T[SparkFilter[Filter stable centroids]], T[SparkFilter[Filter unstable centroids]]]], although it was requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 2 items in 0:00:00.053 (estimated (0:00:00.035 .. 0:00:00.035, p=9.57%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkFilter[Filter unstable centroids]]] in 0:00:00.061 (61 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkFilter[Filter stable centroids]]]:
> In  RddChannel => SparkFilter[Filter stable centroids]
> Out SparkFilter[Filter stable centroids] => RddChannel
[kmeansUnrolled.main()] INFO org.qcri.rheem.spark.execution.SparkExecutor - T[SparkFilter[Filter stable centroids]] was not executed eagerly as requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkFilter[Filter stable centroids]]] in 0:00:00.007 (7 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkUnionAll[2->1, id=505509d0]]].
[kmeansUnrolled.main()] INFO org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 59 from persistence list
[block-manager-slave-async-thread-pool-1] INFO org.apache.spark.storage.BlockManager - Removing RDD 59
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]]:
> In  CollectionChannel => SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
[kmeansUnrolled.main()] INFO org.apache.spark.storage.MemoryStore - Block broadcast_22 stored as values in memory (estimated size 464.0 B, free 358.8 KB)
[kmeansUnrolled.main()] INFO org.apache.spark.storage.MemoryStore - Block broadcast_22_piece0 stored as bytes in memory (estimated size 1036.0 B, free 359.8 KB)
[dispatcher-event-loop-1] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_22_piece0 in memory on localhost:39672 (size: 1036.0 B, free: 1104.3 MB)
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Created broadcast 22 from broadcast at SparkBroadcastOperator.java:46
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - No cardinality available for BroadcastChannel[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]->[T[SparkMap[Tag stable centroids]], T[SparkMap[1+1->1, id=17c658fc]], T[SparkMap[Find nearest centroid]]]], although it was requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 1 items in 0:00:00.004 (estimated (0:00:00.015 .. 0:00:00.015, p=38.74%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]] in 0:00:00.005 (5 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkMap[1+1->1, id=17c658fc]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkUnionAll[2->1, id=505509d0]]]:
> In  RddChannel => SparkUnionAll[2->1, id=505509d0]
> In  RddChannel => SparkUnionAll[2->1, id=505509d0]
> Out SparkUnionAll[2->1, id=505509d0] => RddChannel
[kmeansUnrolled.main()] INFO org.qcri.rheem.spark.execution.SparkExecutor - T[SparkUnionAll[2->1, id=505509d0]] was not executed eagerly as requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkUnionAll[2->1, id=505509d0]]] in 0:00:00.005 (5 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkMap[1+1->1, id=17c658fc]]]:
> In  RddChannel => SparkMap[1+1->1, id=17c658fc]
> In  BroadcastChannel => SparkMap[1+1->1, id=17c658fc]
>     SparkMap[1+1->1, id=17c658fc] => RddChannel => SparkFilter[1->1, id=31fe1e99]
>     SparkFilter[1->1, id=31fe1e99] => RddChannel => SparkMap[Find nearest centroid]
>     SparkMap[Find nearest centroid] => RddChannel => SparkCache[convert out@SparkMap[Find nearest centroid]]
> Out SparkCache[convert out@SparkMap[Find nearest centroid]] => RddChannel
> Out SparkCache[convert out@SparkMap[Find nearest centroid]] => RddChannel
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: foreachPartition at SparkCacheOperator.java:44
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 13 (foreachPartition at SparkCacheOperator.java:44) with 1 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 21 (foreachPartition at SparkCacheOperator.java:44)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 21 (MapPartitionsRDD[69] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_23 stored as values in memory (estimated size 9.1 KB, free 368.9 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_23_piece0 stored as bytes in memory (estimated size 4.2 KB, free 373.1 KB)
[dispatcher-event-loop-2] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_23_piece0 in memory on localhost:39672 (size: 4.2 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 23 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[69] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 21.0 with 1 tasks
[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 21.0 (TID 44, localhost, partition 0,PROCESS_LOCAL, 2152 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 21.0 (TID 44)
[Executor task launch worker-1] INFO org.apache.spark.CacheManager - Partition rdd_69_0 not found, computing it
[Executor task launch worker-1] INFO org.apache.spark.storage.BlockManager - Found block rdd_53_0 locally
[Executor task launch worker-1] INFO org.apache.spark.storage.MemoryStore - Block rdd_69_0 stored as values in memory (estimated size 16.0 B, free 373.1 KB)
[dispatcher-event-loop-4] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_69_0 in memory on localhost:39672 (size: 16.0 B, free: 1104.3 MB)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 21.0 (TID 44). 2719 bytes result sent to driver
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 21.0 (TID 44) in 12 ms on localhost (1/1)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 21.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 21 (foreachPartition at SparkCacheOperator.java:44) finished in 0.014 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 13 finished: foreachPartition at SparkCacheOperator.java:44, took 0.027538 s
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 4 items in 0:00:00.035 (estimated (0:00:00.060 .. 0:00:00.070, p=4.58%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkMap[1+1->1, id=17c658fc]]] in 0:00:00.046 (46 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkReduceBy[Add up points]]].
[kmeansUnrolled.main()] INFO org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 53 from persistence list
[block-manager-slave-async-thread-pool-2] INFO org.apache.spark.storage.BlockManager - Removing RDD 53
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkReduceBy[Add up points]]]:
> In  RddChannel => SparkReduceBy[Add up points]
>     SparkReduceBy[Add up points] => RddChannel => SparkMap[Average points]
>     SparkMap[Average points] => RddChannel => SparkMap[Tag stable centroids]
>     SparkMap[Tag stable centroids] => RddChannel => SparkCache[convert out@SparkMap[Tag stable centroids]]
> Out SparkCache[convert out@SparkMap[Tag stable centroids]] => RddChannel
> Out SparkCache[convert out@SparkMap[Tag stable centroids]] => RddChannel
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - Execution of T[SparkReduceBy[Add up points]] took suspiciously long (0:00:00.012).
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: foreachPartition at SparkCacheOperator.java:44
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 70 (mapToPair at SparkReduceByOperator.java:73)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 14 (foreachPartition at SparkCacheOperator.java:44) with 4 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 23 (foreachPartition at SparkCacheOperator.java:44)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 22)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 22)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 22 (Add up points MapPartitionsRDD[70] at mapToPair at SparkReduceByOperator.java:73), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_24 stored as values in memory (estimated size 10.1 KB, free 379.9 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_24_piece0 stored as bytes in memory (estimated size 4.7 KB, free 384.7 KB)
[dispatcher-event-loop-7] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_24_piece0 in memory on localhost:39672 (size: 4.7 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 24 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 22 (Add up points MapPartitionsRDD[70] at mapToPair at SparkReduceByOperator.java:73)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 22.0 with 1 tasks
[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 22.0 (TID 45, localhost, partition 0,PROCESS_LOCAL, 2141 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 22.0 (TID 45)
[Executor task launch worker-1] INFO org.apache.spark.storage.BlockManager - Found block rdd_69_0 locally
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 22.0 (TID 45). 2351 bytes result sent to driver
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 22.0 (TID 45) in 13 ms on localhost (1/1)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 22.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 22 (mapToPair at SparkReduceByOperator.java:73) finished in 0.013 s
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 23)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 23 (MapPartitionsRDD[75] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_25 stored as values in memory (estimated size 6.9 KB, free 391.6 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_25_piece0 stored as bytes in memory (estimated size 3.7 KB, free 395.3 KB)
[dispatcher-event-loop-6] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_25_piece0 in memory on localhost:39672 (size: 3.7 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 25 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 23 (MapPartitionsRDD[75] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 23.0 with 4 tasks
[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 23.0 (TID 46, localhost, partition 0,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 23.0 (TID 46)
[Executor task launch worker-1] INFO org.apache.spark.CacheManager - Partition rdd_75_0 not found, computing it
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.storage.MemoryStore - Block rdd_75_0 stored as values in memory (estimated size 16.0 B, free 395.3 KB)
[dispatcher-event-loop-0] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_75_0 in memory on localhost:39672 (size: 16.0 B, free: 1104.3 MB)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 23.0 (TID 46). 1805 bytes result sent to driver
[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 23.0 (TID 47, localhost, partition 1,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 23.0 (TID 47)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 23.0 (TID 46) in 11 ms on localhost (1/4)
[Executor task launch worker-1] INFO org.apache.spark.CacheManager - Partition rdd_75_1 not found, computing it
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.storage.MemoryStore - Block rdd_75_1 stored as values in memory (estimated size 16.0 B, free 395.3 KB)
[dispatcher-event-loop-6] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_75_1 in memory on localhost:39672 (size: 16.0 B, free: 1104.3 MB)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 23.0 (TID 47). 1805 bytes result sent to driver
[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 23.0 (TID 48, localhost, partition 2,PROCESS_LOCAL, 1894 bytes)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 23.0 (TID 47) in 11 ms on localhost (2/4)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 2.0 in stage 23.0 (TID 48)
[Executor task launch worker-1] INFO org.apache.spark.CacheManager - Partition rdd_75_2 not found, computing it
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.storage.MemoryStore - Block rdd_75_2 stored as values in memory (estimated size 16.0 B, free 395.3 KB)
[dispatcher-event-loop-0] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_75_2 in memory on localhost:39672 (size: 16.0 B, free: 1104.3 MB)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 2.0 in stage 23.0 (TID 48). 1805 bytes result sent to driver
[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 23.0 (TID 49, localhost, partition 3,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 3.0 in stage 23.0 (TID 49)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 23.0 (TID 48) in 11 ms on localhost (3/4)
[Executor task launch worker-1] INFO org.apache.spark.CacheManager - Partition rdd_75_3 not found, computing it
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[Executor task launch worker-1] INFO org.apache.spark.storage.MemoryStore - Block rdd_75_3 stored as values in memory (estimated size 16.0 B, free 395.3 KB)
[dispatcher-event-loop-6] INFO org.apache.spark.storage.BlockManagerInfo - Added rdd_75_3 in memory on localhost:39672 (size: 16.0 B, free: 1104.3 MB)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 3.0 in stage 23.0 (TID 49). 1805 bytes result sent to driver
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 23.0 (TID 49) in 10 ms on localhost (4/4)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 23.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 23 (foreachPartition at SparkCacheOperator.java:44) finished in 0.042 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 14 finished: foreachPartition at SparkCacheOperator.java:44, took 0.083483 s
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - No cardinality available for RddChannel[T[SparkCache[convert out@SparkMap[Find nearest centroid]]]->[T[SparkReduceBy[Add up points]], T[SparkMap[1+1->1, id=7e17326d]]]], although it was requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 4 items in 0:00:00.092 (estimated (0:00:00.080 .. 0:00:00.090, p=5.90%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkReduceBy[Add up points]]] in 0:00:00.114 (114 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkFilter[Filter unstable centroids]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkFilter[Filter stable centroids]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkFilter[Filter unstable centroids]]]:
> In  RddChannel => SparkFilter[Filter unstable centroids]
>     SparkFilter[Filter unstable centroids] => RddChannel => SparkCollect[convert out@SparkFilter[Filter unstable centroids]]
> Out SparkCollect[convert out@SparkFilter[Filter unstable centroids]] => CollectionChannel
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: collect at SparkCollectOperator.java:43
[dag-scheduler-event-loop] INFO org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 4 is 145 bytes
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 15 (collect at SparkCollectOperator.java:43) with 4 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 25 (collect at SparkCollectOperator.java:43)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 24)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 25 (MapPartitionsRDD[77] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_26 stored as values in memory (estimated size 7.1 KB, free 402.4 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_26_piece0 stored as bytes in memory (estimated size 3.8 KB, free 406.2 KB)
[dispatcher-event-loop-7] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_26_piece0 in memory on localhost:39672 (size: 3.8 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 26 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 25 (MapPartitionsRDD[77] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 25.0 with 4 tasks
[dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 25.0 (TID 50, localhost, partition 0,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 25.0 (TID 50)
[Executor task launch worker-1] INFO org.apache.spark.storage.BlockManager - Found block rdd_75_0 locally
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 25.0 (TID 50). 2079 bytes result sent to driver
[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 25.0 (TID 51, localhost, partition 1,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 25.0 (TID 51)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 25.0 (TID 50) in 8 ms on localhost (1/4)
[Executor task launch worker-1] INFO org.apache.spark.storage.BlockManager - Found block rdd_75_1 locally
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 25.0 (TID 51). 2079 bytes result sent to driver
[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 25.0 (TID 52, localhost, partition 2,PROCESS_LOCAL, 1894 bytes)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 25.0 (TID 51) in 9 ms on localhost (2/4)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 2.0 in stage 25.0 (TID 52)
[Executor task launch worker-1] INFO org.apache.spark.storage.BlockManager - Found block rdd_75_2 locally
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 2.0 in stage 25.0 (TID 52). 2079 bytes result sent to driver
[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 25.0 (TID 53, localhost, partition 3,PROCESS_LOCAL, 1894 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 3.0 in stage 25.0 (TID 53)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 25.0 (TID 52) in 8 ms on localhost (3/4)
[Executor task launch worker-1] INFO org.apache.spark.storage.BlockManager - Found block rdd_75_3 locally
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 3.0 in stage 25.0 (TID 53). 2079 bytes result sent to driver
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 25.0 (TID 53) in 8 ms on localhost (4/4)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 25.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 25 (collect at SparkCollectOperator.java:43) finished in 0.031 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 15 finished: collect at SparkCollectOperator.java:43, took 0.048633 s
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - No cardinality available for RddChannel[T[SparkCache[convert out@SparkMap[Tag stable centroids]]]->[T[SparkFilter[Filter unstable centroids]], T[SparkFilter[Filter stable centroids]]]], although it was requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 2 items in 0:00:00.056 (estimated (0:00:00.035 .. 0:00:00.035, p=7.75%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkFilter[Filter unstable centroids]]] in 0:00:00.062 (62 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkFilter[Filter stable centroids]]]:
> In  RddChannel => SparkFilter[Filter stable centroids]
> Out SparkFilter[Filter stable centroids] => RddChannel
[kmeansUnrolled.main()] INFO org.qcri.rheem.spark.execution.SparkExecutor - T[SparkFilter[Filter stable centroids]] was not executed eagerly as requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkFilter[Filter stable centroids]]] in 0:00:00.005 (5 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkUnionAll[2->1, id=127ead93]]].
[kmeansUnrolled.main()] INFO org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 75 from persistence list
[block-manager-slave-async-thread-pool-1] INFO org.apache.spark.storage.BlockManager - Removing RDD 75
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]]:
> In  CollectionChannel => SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
> Out SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]] => BroadcastChannel
[kmeansUnrolled.main()] INFO org.apache.spark.storage.MemoryStore - Block broadcast_27 stored as values in memory (estimated size 464.0 B, free 406.6 KB)
[kmeansUnrolled.main()] INFO org.apache.spark.storage.MemoryStore - Block broadcast_27_piece0 stored as bytes in memory (estimated size 1036.0 B, free 407.6 KB)
[dispatcher-event-loop-3] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_27_piece0 in memory on localhost:39672 (size: 1036.0 B, free: 1104.3 MB)
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Created broadcast 27 from broadcast at SparkBroadcastOperator.java:46
[kmeansUnrolled.main()] WARN org.qcri.rheem.spark.execution.SparkExecutor - No cardinality available for BroadcastChannel[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]->[T[SparkMap[1+1->1, id=7e17326d]], T[SparkMap[Find nearest centroid]], T[SparkMap[Tag stable centroids]]]], although it was requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 1 items in 0:00:00.007 (estimated (0:00:00.015 .. 0:00:00.015, p=31.38%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkBroadcast[convert out@SparkFilter[Filter unstable centroids]]]] in 0:00:00.008 (8 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkMap[1+1->1, id=7e17326d]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkUnionAll[2->1, id=127ead93]]]:
> In  RddChannel => SparkUnionAll[2->1, id=127ead93]
> In  RddChannel => SparkUnionAll[2->1, id=127ead93]
> Out SparkUnionAll[2->1, id=127ead93] => RddChannel
[kmeansUnrolled.main()] INFO org.qcri.rheem.spark.execution.SparkExecutor - T[SparkUnionAll[2->1, id=127ead93]] was not executed eagerly as requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkUnionAll[2->1, id=127ead93]]] in 0:00:00.004 (4 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkMap[1+1->1, id=7e17326d]]]:
> In  RddChannel => SparkMap[1+1->1, id=7e17326d]
> In  BroadcastChannel => SparkMap[1+1->1, id=7e17326d]
>     SparkMap[1+1->1, id=7e17326d] => RddChannel => SparkFilter[1->1, id=39012a93]
>     SparkFilter[1->1, id=39012a93] => RddChannel => SparkMap[Find nearest centroid]
>     SparkMap[Find nearest centroid] => RddChannel => SparkReduceBy[Add up points]
>     SparkReduceBy[Add up points] => RddChannel => SparkMap[Average points]
>     SparkMap[Average points] => RddChannel => SparkMap[Tag stable centroids]
>     SparkMap[Tag stable centroids] => RddChannel => SparkFilter[Filter stable centroids]
> Out SparkFilter[Filter stable centroids] => RddChannel
[kmeansUnrolled.main()] INFO org.qcri.rheem.spark.execution.SparkExecutor - T[SparkFilter[Filter stable centroids]] was not executed eagerly as requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkMap[1+1->1, id=7e17326d]]] in 0:00:00.025 (25 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkUnionAll[2->1, id=6e725eec]]].
[kmeansUnrolled.main()] INFO org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 69 from persistence list
[block-manager-slave-async-thread-pool-2] INFO org.apache.spark.storage.BlockManager - Removing RDD 69
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkUnionAll[2->1, id=6e725eec]]]:
> In  RddChannel => SparkUnionAll[2->1, id=6e725eec]
> In  RddChannel => SparkUnionAll[2->1, id=6e725eec]
> Out SparkUnionAll[2->1, id=6e725eec] => RddChannel
[kmeansUnrolled.main()] INFO org.qcri.rheem.spark.execution.SparkExecutor - T[SparkUnionAll[2->1, id=6e725eec]] was not executed eagerly as requested.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkUnionAll[2->1, id=6e725eec]]] in 0:00:00.004 (4 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkCount[1->1, id=7329afc6]]].
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkCount[1->1, id=7329afc6]]]:
> In  RddChannel => SparkCount[1->1, id=7329afc6]
> Out SparkCount[1->1, id=7329afc6] => CollectionChannel
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: count at SparkCountOperator.java:59
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 85 (mapToPair at SparkReduceByOperator.java:73)
[dag-scheduler-event-loop] INFO org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 3 is 147 bytes
[dag-scheduler-event-loop] INFO org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 2 is 147 bytes
[dag-scheduler-event-loop] INFO org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 147 bytes
[dag-scheduler-event-loop] INFO org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 145 bytes
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 16 (count at SparkCountOperator.java:59) with 24 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 32 (count at SparkCountOperator.java:59)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 30, ShuffleMapStage 27, ShuffleMapStage 31, ShuffleMapStage 28, ShuffleMapStage 29, ShuffleMapStage 26)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 26)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 26 (Add up points MapPartitionsRDD[85] at mapToPair at SparkReduceByOperator.java:73), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_28 stored as values in memory (estimated size 10.5 KB, free 418.1 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_28_piece0 stored as bytes in memory (estimated size 4.8 KB, free 422.9 KB)
[dispatcher-event-loop-5] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_28_piece0 in memory on localhost:39672 (size: 4.8 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 28 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 26 (Add up points MapPartitionsRDD[85] at mapToPair at SparkReduceByOperator.java:73)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 26.0 with 1 tasks
[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 26.0 (TID 54, localhost, partition 0,ANY, 2141 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 26.0 (TID 54)
[Executor task launch worker-1] INFO org.apache.spark.rdd.HadoopRDD - Input split: hdfs://tenemhead2/data/2dpoints/tmp_kmeans_big.txt:0+14770
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 26.0 (TID 54). 2376 bytes result sent to driver
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 26.0 (TID 54) in 87 ms on localhost (1/1)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 26.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 26 (mapToPair at SparkReduceByOperator.java:73) finished in 0.087 s
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 32)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 32 (MapPartitionsRDD[93] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_29 stored as values in memory (estimated size 13.4 KB, free 436.3 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_29_piece0 stored as bytes in memory (estimated size 5.4 KB, free 441.7 KB)
[dispatcher-event-loop-4] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_29_piece0 in memory on localhost:39672 (size: 5.4 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 29 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 24 missing tasks from ResultStage 32 (MapPartitionsRDD[93] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 32.0 with 24 tasks
[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 32.0 (TID 55, localhost, partition 0,NODE_LOCAL, 2063 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 32.0 (TID 55)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 32.0 (TID 55). 1498 bytes result sent to driver
[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 32.0 (TID 56, localhost, partition 1,NODE_LOCAL, 2063 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 1.0 in stage 32.0 (TID 56)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 32.0 (TID 55) in 16 ms on localhost (1/24)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 1.0 in stage 32.0 (TID 56). 1498 bytes result sent to driver
[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 32.0 (TID 57, localhost, partition 2,NODE_LOCAL, 2063 bytes)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 32.0 (TID 56) in 12 ms on localhost (2/24)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 2.0 in stage 32.0 (TID 57)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 2.0 in stage 32.0 (TID 57). 1498 bytes result sent to driver
[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 32.0 (TID 58, localhost, partition 3,NODE_LOCAL, 2063 bytes)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 32.0 (TID 57) in 13 ms on localhost (3/24)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 3.0 in stage 32.0 (TID 58)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 3.0 in stage 32.0 (TID 58). 1498 bytes result sent to driver
[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 32.0 (TID 59, localhost, partition 4,NODE_LOCAL, 2063 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 4.0 in stage 32.0 (TID 59)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 32.0 (TID 58) in 13 ms on localhost (4/24)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 4.0 in stage 32.0 (TID 59). 1498 bytes result sent to driver
[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 32.0 (TID 60, localhost, partition 5,NODE_LOCAL, 2063 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 5.0 in stage 32.0 (TID 60)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 32.0 (TID 59) in 11 ms on localhost (5/24)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 5.0 in stage 32.0 (TID 60). 1498 bytes result sent to driver
[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 32.0 (TID 61, localhost, partition 6,NODE_LOCAL, 2063 bytes)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 32.0 (TID 60) in 12 ms on localhost (6/24)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 6.0 in stage 32.0 (TID 61)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 6.0 in stage 32.0 (TID 61). 1498 bytes result sent to driver
[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 32.0 (TID 62, localhost, partition 7,NODE_LOCAL, 2063 bytes)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 32.0 (TID 61) in 12 ms on localhost (7/24)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 7.0 in stage 32.0 (TID 62)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 7.0 in stage 32.0 (TID 62). 1498 bytes result sent to driver
[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 32.0 (TID 63, localhost, partition 8,NODE_LOCAL, 2048 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 8.0 in stage 32.0 (TID 63)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 32.0 (TID 62) in 12 ms on localhost (8/24)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 8.0 in stage 32.0 (TID 63). 1498 bytes result sent to driver
[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 32.0 (TID 64, localhost, partition 9,NODE_LOCAL, 2048 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 9.0 in stage 32.0 (TID 64)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 32.0 (TID 63) in 11 ms on localhost (9/24)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 9.0 in stage 32.0 (TID 64). 1498 bytes result sent to driver
[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 32.0 (TID 65, localhost, partition 10,NODE_LOCAL, 2048 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 10.0 in stage 32.0 (TID 65)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 32.0 (TID 64) in 12 ms on localhost (10/24)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 10.0 in stage 32.0 (TID 65). 1498 bytes result sent to driver
[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 32.0 (TID 66, localhost, partition 11,NODE_LOCAL, 2048 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 11.0 in stage 32.0 (TID 66)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 32.0 (TID 65) in 10 ms on localhost (11/24)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 11.0 in stage 32.0 (TID 66). 1498 bytes result sent to driver
[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 12.0 in stage 32.0 (TID 67, localhost, partition 12,NODE_LOCAL, 2033 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 12.0 in stage 32.0 (TID 67)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 32.0 (TID 66) in 10 ms on localhost (12/24)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 12.0 in stage 32.0 (TID 67). 1493 bytes result sent to driver
[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 13.0 in stage 32.0 (TID 68, localhost, partition 13,NODE_LOCAL, 2033 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 13.0 in stage 32.0 (TID 68)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 12.0 in stage 32.0 (TID 67) in 10 ms on localhost (13/24)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 13.0 in stage 32.0 (TID 68). 1493 bytes result sent to driver
[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 14.0 in stage 32.0 (TID 69, localhost, partition 14,NODE_LOCAL, 2033 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 14.0 in stage 32.0 (TID 69)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 13.0 in stage 32.0 (TID 68) in 9 ms on localhost (14/24)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 14.0 in stage 32.0 (TID 69). 1493 bytes result sent to driver
[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 15.0 in stage 32.0 (TID 70, localhost, partition 15,NODE_LOCAL, 2033 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 15.0 in stage 32.0 (TID 70)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 14.0 in stage 32.0 (TID 69) in 9 ms on localhost (15/24)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 15.0 in stage 32.0 (TID 70). 1493 bytes result sent to driver
[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 16.0 in stage 32.0 (TID 71, localhost, partition 16,PROCESS_LOCAL, 2018 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 16.0 in stage 32.0 (TID 71)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 15.0 in stage 32.0 (TID 70) in 10 ms on localhost (16/24)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 16.0 in stage 32.0 (TID 71). 1488 bytes result sent to driver
[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 17.0 in stage 32.0 (TID 72, localhost, partition 17,PROCESS_LOCAL, 2018 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 17.0 in stage 32.0 (TID 72)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 16.0 in stage 32.0 (TID 71) in 8 ms on localhost (17/24)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 17.0 in stage 32.0 (TID 72). 1488 bytes result sent to driver
[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 18.0 in stage 32.0 (TID 73, localhost, partition 18,PROCESS_LOCAL, 2018 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 18.0 in stage 32.0 (TID 73)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 17.0 in stage 32.0 (TID 72) in 8 ms on localhost (18/24)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 18.0 in stage 32.0 (TID 73). 1488 bytes result sent to driver
[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 19.0 in stage 32.0 (TID 74, localhost, partition 19,PROCESS_LOCAL, 2018 bytes)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 18.0 in stage 32.0 (TID 73) in 9 ms on localhost (19/24)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 19.0 in stage 32.0 (TID 74)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 19.0 in stage 32.0 (TID 74). 1488 bytes result sent to driver
[dispatcher-event-loop-3] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 20.0 in stage 32.0 (TID 75, localhost, partition 20,PROCESS_LOCAL, 2003 bytes)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 19.0 in stage 32.0 (TID 74) in 12 ms on localhost (20/24)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 20.0 in stage 32.0 (TID 75)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 20.0 in stage 32.0 (TID 75). 1488 bytes result sent to driver
[dispatcher-event-loop-6] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 21.0 in stage 32.0 (TID 76, localhost, partition 21,PROCESS_LOCAL, 2003 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 21.0 in stage 32.0 (TID 76)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 20.0 in stage 32.0 (TID 75) in 8 ms on localhost (21/24)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 21.0 in stage 32.0 (TID 76). 1488 bytes result sent to driver
[dispatcher-event-loop-7] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 22.0 in stage 32.0 (TID 77, localhost, partition 22,PROCESS_LOCAL, 2003 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 22.0 in stage 32.0 (TID 77)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 21.0 in stage 32.0 (TID 76) in 8 ms on localhost (22/24)
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 22.0 in stage 32.0 (TID 77). 1488 bytes result sent to driver
[dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 23.0 in stage 32.0 (TID 78, localhost, partition 23,PROCESS_LOCAL, 2003 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 23.0 in stage 32.0 (TID 78)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 22.0 in stage 32.0 (TID 77) in 29 ms on localhost (23/24)
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 31
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 1 blocks
[Executor task launch worker-1] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 23.0 in stage 32.0 (TID 78). 1488 bytes result sent to driver
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 23.0 in stage 32.0 (TID 78) in 11 ms on localhost (24/24)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 32.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 32 (count at SparkCountOperator.java:59) finished in 0.262 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 16 finished: count at SparkCountOperator.java:59, took 0.409968 s
[dispatcher-event-loop-5] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on localhost:39672 in memory (size: 3.7 KB, free: 1104.3 MB)
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 16
[dispatcher-event-loop-1] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on localhost:39672 in memory (size: 4.4 KB, free: 1104.3 MB)
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 15
[dispatcher-event-loop-2] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on localhost:39672 in memory (size: 3.9 KB, free: 1104.3 MB)
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 13
[dispatcher-event-loop-0] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:39672 in memory (size: 3.8 KB, free: 1104.3 MB)
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 11
[dispatcher-event-loop-3] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:39672 in memory (size: 3.7 KB, free: 1104.3 MB)
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 18 items in 0:00:00.426 (estimated (0:00:00.305 .. 0:00:00.325, p=3.34%)).
[dispatcher-event-loop-5] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_16_piece0 on localhost:39672 in memory (size: 3.8 KB, free: 1104.3 MB)
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkCount[1->1, id=7329afc6]]] in 0:00:00.433 (433 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Activating ExecutionStage[T[SparkCollectionSource[convert out@SparkCount[1->1, id=7329afc6]]]].
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 27
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Having SparkExecutor[0] execute ExecutionStage[T[SparkCollectionSource[convert out@SparkCount[1->1, id=7329afc6]]]]:
> In  CollectionChannel => SparkCollectionSource[convert out@SparkCount[1->1, id=7329afc6]]
>     SparkCollectionSource[convert out@SparkCount[1->1, id=7329afc6]] => RddChannel => SparkLocalCallbackSink[collect()]
[dispatcher-event-loop-1] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_15_piece0 on localhost:39672 in memory (size: 3.7 KB, free: 1104.3 MB)
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 25
[dispatcher-event-loop-2] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_14_piece0 on localhost:39672 in memory (size: 4.5 KB, free: 1104.3 MB)
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 24
[dispatcher-event-loop-0] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_13_piece0 on localhost:39672 in memory (size: 4.1 KB, free: 1104.3 MB)
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 22
[dispatcher-event-loop-3] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on localhost:39672 in memory (size: 3.8 KB, free: 1104.3 MB)
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 18
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 8
[dispatcher-event-loop-5] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:39672 in memory (size: 4.1 KB, free: 1104.3 MB)
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 7
[dispatcher-event-loop-1] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:39672 in memory (size: 3.6 KB, free: 1104.3 MB)
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 5
[dispatcher-event-loop-2] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_28_piece0 on localhost:39672 in memory (size: 4.8 KB, free: 1104.3 MB)
[dispatcher-event-loop-0] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_26_piece0 on localhost:39672 in memory (size: 3.8 KB, free: 1104.3 MB)
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 45
[dispatcher-event-loop-3] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_25_piece0 on localhost:39672 in memory (size: 3.7 KB, free: 1104.3 MB)
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 43
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: hasNext at Iterator.java:115
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 17 (hasNext at Iterator.java:115) with 1 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 33 (hasNext at Iterator.java:115)
[dispatcher-event-loop-5] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_24_piece0 on localhost:39672 in memory (size: 4.7 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 33 (MapPartitionsRDD[95] at filter at RddChannel.java:71), which has no missing parents
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 42
[dispatcher-event-loop-1] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_23_piece0 on localhost:39672 in memory (size: 4.2 KB, free: 1104.3 MB)
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 40
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_30 stored as values in memory (estimated size 3.3 KB, free 233.5 KB)
[dispatcher-event-loop-2] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_21_piece0 on localhost:39672 in memory (size: 3.8 KB, free: 1104.3 MB)
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 36
[dispatcher-event-loop-0] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_20_piece0 on localhost:39672 in memory (size: 3.7 KB, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_30_piece0 stored as bytes in memory (estimated size 2003.0 B, free 231.7 KB)
[dispatcher-event-loop-7] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_30_piece0 in memory on localhost:39672 (size: 2003.0 B, free: 1104.3 MB)
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 34
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 30 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[95] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 33.0 with 1 tasks
[dispatcher-event-loop-4] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_19_piece0 on localhost:39672 in memory (size: 4.6 KB, free: 1104.3 MB)
[Spark Context Cleaner] INFO org.apache.spark.ContextCleaner - Cleaned accumulator 33
[dispatcher-event-loop-2] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 33.0 (TID 79, localhost, partition 0,PROCESS_LOCAL, 2080 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 33.0 (TID 79)
[dispatcher-event-loop-7] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_18_piece0 on localhost:39672 in memory (size: 4.1 KB, free: 1104.3 MB)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 33.0 (TID 79). 975 bytes result sent to driver
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 33.0 (TID 79) in 5 ms on localhost (1/1)
[task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 33.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 33 (hasNext at Iterator.java:115) finished in 0.006 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 17 finished: hasNext at Iterator.java:115, took 0.014741 s
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: hasNext at Iterator.java:115
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 18 (hasNext at Iterator.java:115) with 1 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 34 (hasNext at Iterator.java:115)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 34 (MapPartitionsRDD[95] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_31 stored as values in memory (estimated size 3.3 KB, free 201.3 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_31_piece0 stored as bytes in memory (estimated size 2003.0 B, free 203.2 KB)
[dispatcher-event-loop-1] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_31_piece0 in memory on localhost:39672 (size: 2003.0 B, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 31 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[95] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 34.0 with 1 tasks
[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 34.0 (TID 80, localhost, partition 1,PROCESS_LOCAL, 2080 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 34.0 (TID 80)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 34.0 (TID 80). 975 bytes result sent to driver
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 34.0 (TID 80) in 4 ms on localhost (1/1)
[task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 34.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 34 (hasNext at Iterator.java:115) finished in 0.005 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 18 finished: hasNext at Iterator.java:115, took 0.010868 s
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: hasNext at Iterator.java:115
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 19 (hasNext at Iterator.java:115) with 1 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 35 (hasNext at Iterator.java:115)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 35 (MapPartitionsRDD[95] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_32 stored as values in memory (estimated size 3.3 KB, free 206.5 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_32_piece0 stored as bytes in memory (estimated size 2003.0 B, free 208.5 KB)
[dispatcher-event-loop-0] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_32_piece0 in memory on localhost:39672 (size: 2003.0 B, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 32 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[95] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 35.0 with 1 tasks
[dispatcher-event-loop-5] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 35.0 (TID 81, localhost, partition 2,PROCESS_LOCAL, 2080 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 35.0 (TID 81)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 35.0 (TID 81). 975 bytes result sent to driver
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 35.0 (TID 81) in 4 ms on localhost (1/1)
[task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 35.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 35 (hasNext at Iterator.java:115) finished in 0.005 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 19 finished: hasNext at Iterator.java:115, took 0.012189 s
[kmeansUnrolled.main()] INFO org.apache.spark.SparkContext - Starting job: hasNext at Iterator.java:115
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 20 (hasNext at Iterator.java:115) with 1 output partitions
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 36 (hasNext at Iterator.java:115)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 36 (MapPartitionsRDD[95] at filter at RddChannel.java:71), which has no missing parents
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_33 stored as values in memory (estimated size 3.3 KB, free 211.8 KB)
[dag-scheduler-event-loop] INFO org.apache.spark.storage.MemoryStore - Block broadcast_33_piece0 stored as bytes in memory (estimated size 2003.0 B, free 213.7 KB)
[dispatcher-event-loop-1] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_33_piece0 in memory on localhost:39672 (size: 2003.0 B, free: 1104.3 MB)
[dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 33 from broadcast at DAGScheduler.scala:1006
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[95] at filter at RddChannel.java:71)
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 36.0 with 1 tasks
[dispatcher-event-loop-4] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 36.0 (TID 82, localhost, partition 3,PROCESS_LOCAL, 2094 bytes)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 36.0 (TID 82)
[Executor task launch worker-1] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 36.0 (TID 82). 1053 bytes result sent to driver
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 36.0 (TID 82) in 4 ms on localhost (1/1)
[task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 36.0, whose tasks have all completed, from pool 
[dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 36 (hasNext at Iterator.java:115) finished in 0.004 s
[kmeansUnrolled.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 20 finished: hasNext at Iterator.java:115, took 0.011407 s
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 2 items in 0:00:00.080 (estimated (0:00:00.025 .. 0:00:00.025, p=90.00%)).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed ExecutionStage[T[SparkCollectionSource[convert out@SparkCount[1->1, id=7329afc6]]]] in 0:00:00.085 (85 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.platform.CrossPlatformExecutor - Executed 36 stages in 0:00:08.932 (8932 ms).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkMap[Create points]'s output 0 from (948..1,047, 95.00%) to (998..998, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkMap[Find nearest centroid - iteration zero]'s output 0 from (948..1,047, 95.00%) to (998..998, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkMap[Tag stable centroids - iteration zero]'s output 0 from (100..100, 100.00%) to (99..99, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkFilter[Filter unstable centroids - iteration zero]'s output 0 from (10..100, 90.00%) to (89..89, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkMap[Find nearest centroid]'s output 0 from (94..1,047, 81.00%) to (877..877, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkMap[Tag stable centroids]'s output 0 from (100..100, 90.00%) to (89..89, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkFilter[Filter unstable centroids]'s output 0 from (10..100, 81.00%) to (43..43, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkMap[Find nearest centroid]'s output 0 from (9..1,047, 72.90%) to (438..438, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkMap[Tag stable centroids]'s output 0 from (100..100, 81.00%) to (43..43, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkFilter[Filter unstable centroids]'s output 0 from (10..100, 72.90%) to (10..10, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkMap[Find nearest centroid]'s output 0 from (0..1,047, 65.61%) to (64..64, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkMap[Tag stable centroids]'s output 0 from (100..100, 72.90%) to (10..10, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkFilter[Filter unstable centroids]'s output 0 from (10..100, 65.61%) to (0..0, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkMap[Find nearest centroid]'s output 0 from (0..1,047, 59.05%) to (0..0, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkMap[Tag stable centroids]'s output 0 from (100..100, 65.61%) to (0..0, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkFilter[Filter unstable centroids]'s output 0 from (10..100, 59.05%) to (0..0, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkFilter[Filter stable centroids - iteration zero]'s output 0 from (10..100, 90.00%) to (10..10, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkFilter[Filter stable centroids]'s output 0 from (10..100, 81.00%) to (46..46, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkUnionAll[2->1, id=2ef8007e]'s output 0 from (20..200, 81.00%) to (56..56, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkFilter[Filter stable centroids]'s output 0 from (10..100, 72.90%) to (33..33, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkUnionAll[2->1, id=77096cd7]'s output 0 from (30..300, 72.90%) to (89..89, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkFilter[Filter stable centroids]'s output 0 from (10..100, 65.61%) to (10..10, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkUnionAll[2->1, id=505509d0]'s output 0 from (40..400, 65.61%) to (99..99, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkFilter[Filter stable centroids]'s output 0 from (10..100, 59.05%) to (0..0, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkUnionAll[2->1, id=127ead93]'s output 0 from (50..500, 59.05%) to (99..99, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkFilter[Filter stable centroids]'s output 0 from (10..100, 53.14%) to (0..0, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.optimizer.cardinality.CardinalityEstimatorManager - Updating cardinality of SparkUnionAll[2->1, id=6e725eec]'s output 0 from (60..600, 53.14%) to (99..99, 100.00%).
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.profiling.CardinalityRepository - Storing cardinalities at /home/jonas.kemper/.rheem/cardinalities.json.
[kmeansUnrolled.main()] WARN org.qcri.rheem.core.profiling.CardinalityRepository - Cardinality repository currently disabled.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.profiling.ExecutionLog - Curating execution log at /home/jonas.kemper/.rheem/executions.json.
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.api.Job - Accumulated execution time: 0:00:08.999 (8999 ms) (effective: 0:00:03.621 (3621 ms), overhead: 0:00:05.378 (5378 ms))
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.api.Job - Estimated execution time (plan 1): (0:00:06.056 .. 0:00:06.106, p=3.34%)
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.api.Job - Accumulated costs: 3,621.00 .. 3,621.00
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.api.Job - Estimated costs (plan 1): (6,056.00..6,106.00 ~ 3.3%)
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.api.Job - Plan metrics: 55 virtual operators, 55 execution operators, 55 alternatives, 1 combinations
[kmeansUnrolled.main()] INFO org.qcri.rheem.core.api.Job - StopWatch results:
* Optimization                            - 0:00:03.289
  * Prepare                               - 0:00:00.309
    * Prune&Isolate                       - 0:00:00.070
    * Transformations                     - 0:00:00.239
    * Sanity                              - 0:00:00.000
  * Cardinality&Load Estimation           - 0:00:02.637
    * Create OptimizationContext          - 0:00:00.023
    * Create CardinalityEstimationManager - 0:00:00.002
    * Push Estimation                     - 0:00:02.612
      * Estimate source cardinalities     - 0:00:02.423
  * Create Initial Execution Plan         - 0:00:00.342
    * Enumerate                           - 0:00:00.205
      * Concatenation                     - 0:00:00.115
        * Channel Conversion              - 0:00:00.091
      * Prune                             - 0:00:00.001
    * Pick Best Plan                      - 0:00:00.058
    * Split Stages                        - 0:00:00.060
* Execution                               - 0:00:08.999
  * Execution 0                           - 0:00:08.999
    * Execute                             - 0:00:08.943
* Post-processing                         - 0:00:00.105
  * Log measurements                      - 0:00:00.104
  * Release Resources                     - 0:00:00.001
StableCentroids Buffer(99)
[SparkListenerBus] ERROR org.apache.spark.util.Utils - uncaught error in thread SparkListenerBus, stopping SparkContext
[Spark Context Cleaner] ERROR org.apache.spark.ContextCleaner - Error in cleaning thread
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:176)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1181)
	at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:173)
	at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:68)
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AsynchronousListenerBus.scala:66)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:64)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1181)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63)
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/api,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/static,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/executors/json,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/executors,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/environment/json,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/environment,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/storage/json,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/storage,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/stages/json,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/stages,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
[SparkListenerBus] INFO org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/jobs,null}
[SparkListenerBus] INFO org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://172.16.21.111:4040
[WARNING] thread Thread[IPC Parameter Sending Thread #0,5,kmeansUnrolled] was interrupted but is still alive after waiting at least 14993msecs
[WARNING] thread Thread[IPC Parameter Sending Thread #0,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[dispatcher-event-loop-0,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[dispatcher-event-loop-1,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[dispatcher-event-loop-2,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[dispatcher-event-loop-3,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[dispatcher-event-loop-4,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[dispatcher-event-loop-5,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[dispatcher-event-loop-6,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[dispatcher-event-loop-7,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[shuffle-server-0,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[sparkDriverActorSystem-scheduler-1,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[sparkDriverActorSystem-akka.actor.default-dispatcher-2,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[sparkDriverActorSystem-akka.actor.default-dispatcher-3,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[sparkDriverActorSystem-akka.actor.default-dispatcher-4,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[sparkDriverActorSystem-akka.remote.default-remote-dispatcher-5,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[sparkDriverActorSystem-akka.remote.default-remote-dispatcher-6,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[New I/O worker #1,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[New I/O worker #2,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[New I/O boss #3,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[New I/O worker #4,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[New I/O worker #5,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[New I/O server boss #6,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[MAP_OUTPUT_TRACKER cleanup timer,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[BLOCK_MANAGER cleanup timer,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[BROADCAST_VARS cleanup timer,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[heartbeat-receiver-event-loop-thread,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[netty-rpc-env-timeout,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[Timer-0,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[driver-heartbeater,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[shuffle-server-0,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[SparkListenerBus,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[Executor task launch worker-0,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[task-result-getter-0,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[task-result-getter-1,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[task-result-getter-2,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[task-result-getter-3,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-0,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-0,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-1,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-2,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-1,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] thread Thread[Executor task launch worker-1,5,kmeansUnrolled] will linger despite being asked to die via interruption
[WARNING] NOTE: 42 thread(s) did not finish despite being asked to  via interruption. This is not a problem with exec:java, it is a problem with the running code. Although not serious, it should be remedied.
[WARNING] Couldn't destroy threadgroup org.codehaus.mojo.exec.ExecJavaMojo$IsolatedThreadGroup[name=kmeansUnrolled,maxpri=10]
java.lang.IllegalThreadStateException
	at java.lang.ThreadGroup.destroy(ThreadGroup.java:778)
	at org.codehaus.mojo.exec.ExecJavaMojo.execute(ExecJavaMojo.java:321)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 31.488 s
[INFO] Finished at: 2017-03-26T23:12:57+02:00
[INFO] Final Memory: 27M/445M
[INFO] ------------------------------------------------------------------------
[Thread-2] INFO org.apache.spark.storage.DiskBlockManager - Shutdown hook called
[Thread-2] INFO org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[Thread-2] INFO org.apache.spark.util.ShutdownHookManager - Deleting directory /data/tmp/spark-c48e2ece-d59e-447b-b7a0-88db9192eabe/userFiles-d063d890-8a28-4ad8-a7ba-f646cb880b84
[Thread-2] INFO org.apache.spark.util.ShutdownHookManager - Deleting directory /data/tmp/spark-c48e2ece-d59e-447b-b7a0-88db9192eabe
